{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrate experiments on basic MLP and its Input Ouput Convex(IOC) counterpart on both CIFAR-10 and MNIST datasets. Here's a summary of the key components and experiments covered:\n",
        "\n",
        "Exp 1: **NN and IOC-NN On CIFAR10**:\n",
        "   - Implementing a simple MLP architecture using PyTorch.\n",
        "   - Constructing an IOC-NN architecture, including weight exponentiation and other modifications to enforce convexity constraints.\n",
        "   - Evaluating training and validation performance, and testing accuracy.\n",
        "\n",
        "Exp 2: **Training on Duplicate Free data (ciFAIR10)**:\n",
        "   - We use the same implementation of NN and IOC-NN on ciFAIR10 data set which doesn't have duplicate images in its test set\n",
        "\n",
        "Exp 3: **Ensembles of Binary Experts**:\n",
        "\n",
        "- **Models:** BinaryExpert (binary classifier) and GatingNetwork (weight predictor).\n",
        "- **Ensemble:** Combination of BinaryExpert models weighted by GatingNetwork predictions.\n",
        "\n",
        "Exp 4: **BoostedEnsemble**:\n",
        "\n",
        "- **Initialization:** ExpertEnsemble setup with base model and expert count.\n",
        "- **Training:** Experts trained separately with bootstrapped data, updating weights based on performance.\n",
        "      \n",
        "Exp 5. **MNIST**:\n",
        "  - we implement the same architectures on MNIST data set\n",
        "   "
      ],
      "metadata": {
        "id": "wbB_IFIQui4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp.1 - NN and IOC-NN On CIFAR10"
      ],
      "metadata": {
        "id": "kwNr_dZj2OBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOI-L9mOF2Db"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqDm6e1Qy8dk"
      },
      "source": [
        "## NN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "cfbKWKtYZUL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyUDa8g_ohaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af28ac46-cae0-4d8a-e943-7c273d4ebf6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 72241876.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Define data transformation: Convert images to tensors\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 train and test datasets\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define batch size for data loaders\n",
        "batch_size = 64\n",
        "\n",
        "# Split train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "# Randomly split train data into train and validation sets\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Create data loaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "_5ufa_PbZYkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ4XLjxntRiL"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define layers of the neural network\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Flatten the input image tensor\n",
        "            nn.Linear(32*32*3, 800),\n",
        "            # Hidden Layer 1: Fully connected layer with 32*32*3 input features and 800 output features\n",
        "            nn.BatchNorm1d(800),\n",
        "            # Batch normalization for better training stability\n",
        "            nn.ReLU(inplace=True),\n",
        "            # ReLU activation function\n",
        "\n",
        "            nn.Linear(800, 800),\n",
        "            # Hidden Layer 2: Fully connected layer with 800 input features and 800 output features\n",
        "            nn.BatchNorm1d(800),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(800, 800),\n",
        "            # Hidden Layer 3: Fully connected layer with 800 input features and 800 output features\n",
        "            nn.BatchNorm1d(800),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(800, 10),\n",
        "            # Output Layer: Fully connected layer with 800 input features and 10 output features (for classification)\n",
        "            nn.Softmax(dim=1)\n",
        "            # Softmax activation function to obtain probabilities for each class\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the layers\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPmXd-SpzDAX"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = NN()  # Create an instance of the neural network\n",
        "loss_fun = nn.CrossEntropyLoss()  # Define the loss function\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)  # Define the optimizer\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 50\n",
        "\n",
        "# Main training loop\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "\n",
        "    current_loss = []  # List to store current epoch's losses\n",
        "    current_acc = []   # List to store current epoch's accuracies\n",
        "    val_acc = []       # List to store validation accuracies\n",
        "\n",
        "    # Training\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        outputs = mlp(inputs)  # Forward pass\n",
        "\n",
        "        loss = loss_fun(outputs, targets)  # Calculate the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "\n",
        "        current_loss.append(loss.item())  # Append current loss to the list\n",
        "\n",
        "        # Calculate and append accuracy\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item() / targets.shape[0]\n",
        "        current_acc.append(acc)\n",
        "\n",
        "    # Validation\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = mlp(inputs)  # Forward pass for validation\n",
        "\n",
        "        # Calculate and append validation accuracy\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item() / targets.shape[0]\n",
        "        val_acc.append(acc)\n",
        "\n",
        "    # Performance Evaluation\n",
        "    avg_train_loss = sum(current_loss) / len(current_loss)\n",
        "    avg_train_acc = sum(current_acc) / len(current_acc)\n",
        "    avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    print('Loss : %.3f' % avg_train_loss)\n",
        "    print('Training Accuracy : %.3f' % avg_train_acc)\n",
        "    print('Validation Accuracy : %.3f' % avg_val_acc)\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = avg_val_acc - prev_val_acc\n",
        "    print('Update : %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if 0 < update < 1e-4:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = avg_val_acc\n"
      ],
      "metadata": {
        "id": "AWcJzWmarAVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "LKNzxGclZiba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmyaV2tIl9gB",
        "outputId": "f58a5c64-a396-4b43-c243-cd220e9655a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_Acc  : 0.543\n"
          ]
        }
      ],
      "source": [
        "test_acc = []\n",
        "\n",
        "# Iterate over the test dataset\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    inputs, targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs, 1).indices\n",
        "    acc = (targets == pred).sum().item()\n",
        "    test_acc.append(acc / targets.shape[0])\n",
        "\n",
        "# Calculate average test accuracy\n",
        "avg_test_acc = sum(test_acc) / len(test_acc)\n",
        "print('Test_Acc: %.3f' % avg_test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H66GupBRtbQc"
      },
      "source": [
        "## IOC - NN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "UNXT0-eLZ1dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WhiteningTransform:\n",
        "    def __init__(self):\n",
        "        self.whitening = None  # Initialize the whitening matrix to None\n",
        "\n",
        "    def fit(self, data):\n",
        "        mean = data.mean(dim=0)  # Compute the mean of the data along each dimension\n",
        "        centered_data = data - mean  # Center the data by subtracting the mean\n",
        "\n",
        "        cov_matrix = torch.mm(centered_data.t(), centered_data) / data.size(0)  # Compute the covariance matrix\n",
        "        U, S, V = torch.svd(cov_matrix)  # Perform singular value decomposition (SVD) on the covariance matrix\n",
        "        self.whitening = torch.mm(centered_data, U) / torch.sqrt(S + 1e-10)  # Compute the whitening matrix\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.whitening is not None:\n",
        "            # Apply whitening transformation to the input data\n",
        "            x = self.whitening.t() @ x.reshape(x.size(0), -1).t()\n",
        "            x = x.t().reshape(x.size(0), 3, 32, 32)  # Reshape the transformed data back to the original shape\n",
        "            return x\n",
        "        return x\n",
        "\n",
        "# Creating a transform object with whitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image or numpy.ndarray to tensor\n",
        "    WhiteningTransform()  # Apply whitening transformation\n",
        "])"
      ],
      "metadata": {
        "id": "frQiNyWx3OJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CIFAR-10 dataset with whitening transformation\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "# Randomly split train data into train and validation sets\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "H5LsppEAMwtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78accea1-0c1b-4fcc-fc84-fcb2c7a4f525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "dJXgz0RSZ4H9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen7qqFAn_I8"
      },
      "outputs": [],
      "source": [
        "class IOC_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the first hidden layer with increased number of nodes\n",
        "        self.first_hidden_layer = nn.Linear(32*32*3, 1000)  # Hidden Layer 1\n",
        "\n",
        "        # Define the rest of the layers using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),  # Flatten the input image tensor\n",
        "            self.first_hidden_layer,\n",
        "            nn.BatchNorm1d(1000),\n",
        "            # Batch normalization for better training stability\n",
        "            nn.ELU(inplace=True),\n",
        "            # ELU activation function\n",
        "\n",
        "            nn.Linear(1000, 800),\n",
        "            # Hidden Layer 2: Fully connected layer with 1000 input features and 800 output features\n",
        "            nn.BatchNorm1d(800),\n",
        "            nn.ELU(inplace=True),\n",
        "\n",
        "            nn.Linear(800, 800),\n",
        "            # Hidden Layer 3: Fully connected layer with 800 input features and 800 output features\n",
        "            nn.BatchNorm1d(800),\n",
        "            nn.ELU(inplace=True),\n",
        "\n",
        "            nn.Linear(800, 10),\n",
        "            # Output Layer: Fully connected layer with 800 input features and 10 output features (for classification)\n",
        "            nn.Softmax(dim=1)\n",
        "             # Softmax activation function to obtain probabilities for each class\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightExponentiation(object):\n",
        "    def __init__(self, epsilon=5):\n",
        "        self.epsilon = epsilon\n",
        "        # Epsilon value for constraining exponentiation of weights\n",
        "\n",
        "    def __call__(self, module):\n",
        "        # Check if the module has weights\n",
        "        if hasattr(module, 'weight'):\n",
        "            # Exclude the first_hidden_layer\n",
        "            if \"first_hidden_layer\" not in module.__str__():\n",
        "                w = module.weight.data\n",
        "                # Get the weights of the module\n",
        "                # Perform exponentiation operation on negative weights\n",
        "                w[w < 0] = torch.exp(w[w < 0] - self.epsilon)\n",
        "                module.weight.data = w  # Store the updated weights"
      ],
      "metadata": {
        "id": "q6y3mKNO3r0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "JbxZsR3haUKO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m64n9YditWVn",
        "outputId": "551467ae-6fb9-44dd-946b-1835382b3ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss : 2.261\n",
            "Training_Acc  : 0.165\n",
            "Validation_Acc  : 0.186\n",
            "update: 0.1864\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Loss : 2.232\n",
            "Training_Acc  : 0.209\n",
            "Validation_Acc  : 0.211\n",
            "update: 0.0246\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Loss : 2.220\n",
            "Training_Acc  : 0.223\n",
            "Validation_Acc  : 0.222\n",
            "update: 0.0106\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Loss : 2.215\n",
            "Training_Acc  : 0.229\n",
            "Validation_Acc  : 0.225\n",
            "update: 0.0035\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Loss : 2.209\n",
            "Training_Acc  : 0.237\n",
            "Validation_Acc  : 0.235\n",
            "update: 0.0101\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Loss : 2.202\n",
            "Training_Acc  : 0.246\n",
            "Validation_Acc  : 0.245\n",
            "update: 0.0096\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Loss : 2.196\n",
            "Training_Acc  : 0.252\n",
            "Validation_Acc  : 0.251\n",
            "update: 0.0068\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Loss : 2.192\n",
            "Training_Acc  : 0.257\n",
            "Validation_Acc  : 0.250\n",
            "update: -0.0015\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Loss : 2.189\n",
            "Training_Acc  : 0.260\n",
            "Validation_Acc  : 0.255\n",
            "update: 0.0054\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Loss : 2.187\n",
            "Training_Acc  : 0.263\n",
            "Validation_Acc  : 0.258\n",
            "update: 0.0026\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Loss : 2.184\n",
            "Training_Acc  : 0.264\n",
            "Validation_Acc  : 0.261\n",
            "update: 0.0033\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Loss : 2.182\n",
            "Training_Acc  : 0.266\n",
            "Validation_Acc  : 0.263\n",
            "update: 0.0020\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Loss : 2.180\n",
            "Training_Acc  : 0.269\n",
            "Validation_Acc  : 0.268\n",
            "update: 0.0046\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Loss : 2.178\n",
            "Training_Acc  : 0.272\n",
            "Validation_Acc  : 0.263\n",
            "update: -0.0053\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Loss : 2.178\n",
            "Training_Acc  : 0.271\n",
            "Validation_Acc  : 0.268\n",
            "update: 0.0052\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Loss : 2.177\n",
            "Training_Acc  : 0.273\n",
            "Validation_Acc  : 0.265\n",
            "update: -0.0024\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Loss : 2.175\n",
            "Training_Acc  : 0.276\n",
            "Validation_Acc  : 0.270\n",
            "update: 0.0047\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Loss : 2.174\n",
            "Training_Acc  : 0.277\n",
            "Validation_Acc  : 0.272\n",
            "update: 0.0016\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Loss : 2.172\n",
            "Training_Acc  : 0.279\n",
            "Validation_Acc  : 0.273\n",
            "update: 0.0011\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Loss : 2.172\n",
            "Training_Acc  : 0.278\n",
            "Validation_Acc  : 0.277\n",
            "update: 0.0042\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Loss : 2.171\n",
            "Training_Acc  : 0.279\n",
            "Validation_Acc  : 0.278\n",
            "update: 0.0009\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Loss : 2.170\n",
            "Training_Acc  : 0.281\n",
            "Validation_Acc  : 0.275\n",
            "update: -0.0024\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Loss : 2.169\n",
            "Training_Acc  : 0.284\n",
            "Validation_Acc  : 0.279\n",
            "update: 0.0037\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Loss : 2.168\n",
            "Training_Acc  : 0.284\n",
            "Validation_Acc  : 0.276\n",
            "update: -0.0026\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Loss : 2.167\n",
            "Training_Acc  : 0.283\n",
            "Validation_Acc  : 0.283\n",
            "update: 0.0069\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Loss : 2.166\n",
            "Training_Acc  : 0.284\n",
            "Validation_Acc  : 0.280\n",
            "update: -0.0037\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Loss : 2.165\n",
            "Training_Acc  : 0.287\n",
            "Validation_Acc  : 0.281\n",
            "update: 0.0010\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Loss : 2.164\n",
            "Training_Acc  : 0.287\n",
            "Validation_Acc  : 0.284\n",
            "update: 0.0034\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Loss : 2.163\n",
            "Training_Acc  : 0.288\n",
            "Validation_Acc  : 0.283\n",
            "update: -0.0014\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Loss : 2.162\n",
            "Training_Acc  : 0.292\n",
            "Validation_Acc  : 0.284\n",
            "update: 0.0010\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Loss : 2.161\n",
            "Training_Acc  : 0.290\n",
            "Validation_Acc  : 0.288\n",
            "update: 0.0039\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Loss : 2.161\n",
            "Training_Acc  : 0.293\n",
            "Validation_Acc  : 0.288\n",
            "update: 0.0001\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Loss : 2.160\n",
            "Training_Acc  : 0.294\n",
            "Validation_Acc  : 0.288\n",
            "update: 0.0008\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Loss : 2.159\n",
            "Training_Acc  : 0.295\n",
            "Validation_Acc  : 0.287\n",
            "update: -0.0011\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Loss : 2.159\n",
            "Training_Acc  : 0.295\n",
            "Validation_Acc  : 0.287\n",
            "update: -0.0008\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Loss : 2.158\n",
            "Training_Acc  : 0.296\n",
            "Validation_Acc  : 0.288\n",
            "update: 0.0019\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Loss : 2.157\n",
            "Training_Acc  : 0.295\n",
            "Validation_Acc  : 0.288\n",
            "update: -0.0004\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Loss : 2.157\n",
            "Training_Acc  : 0.296\n",
            "Validation_Acc  : 0.291\n",
            "update: 0.0028\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Loss : 2.157\n",
            "Training_Acc  : 0.298\n",
            "Validation_Acc  : 0.288\n",
            "update: -0.0024\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Loss : 2.156\n",
            "Training_Acc  : 0.296\n",
            "Validation_Acc  : 0.294\n",
            "update: 0.0054\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Loss : 2.155\n",
            "Training_Acc  : 0.298\n",
            "Validation_Acc  : 0.294\n",
            "update: 0.0006\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Loss : 2.154\n",
            "Training_Acc  : 0.300\n",
            "Validation_Acc  : 0.291\n",
            "update: -0.0034\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Loss : 2.154\n",
            "Training_Acc  : 0.300\n",
            "Validation_Acc  : 0.293\n",
            "update: 0.0023\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Loss : 2.153\n",
            "Training_Acc  : 0.301\n",
            "Validation_Acc  : 0.294\n",
            "update: 0.0008\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Loss : 2.152\n",
            "Training_Acc  : 0.302\n",
            "Validation_Acc  : 0.291\n",
            "update: -0.0031\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Loss : 2.152\n",
            "Training_Acc  : 0.303\n",
            "Validation_Acc  : 0.296\n",
            "update: 0.0047\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Loss : 2.151\n",
            "Training_Acc  : 0.302\n",
            "Validation_Acc  : 0.298\n",
            "update: 0.0025\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Loss : 2.150\n",
            "Training_Acc  : 0.304\n",
            "Validation_Acc  : 0.297\n",
            "update: -0.0009\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Loss : 2.149\n",
            "Training_Acc  : 0.305\n",
            "Validation_Acc  : 0.299\n",
            "update: 0.0014\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Loss : 2.149\n",
            "Training_Acc  : 0.305\n",
            "Validation_Acc  : 0.300\n",
            "update: 0.0018\n",
            "-------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "ioc_nn = IOC_NN()  # Instantiate the IOC_NN model\n",
        "loss_fun = nn.CrossEntropyLoss()  # Define the loss function\n",
        "optimizer = torch.optim.Adam(ioc_nn.parameters(), lr=1e-4, betas=(0.9, 0.9))\n",
        " # Define the optimizer with adjusted beta values for Adam optimizer\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 50\n",
        "\n",
        "# Main training loop\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "\n",
        "    current_loss = []  # List to store current epoch's losses\n",
        "    current_acc = []   # List to store current epoch's accuracies\n",
        "    val_acc = []       # List to store validation accuracies\n",
        "\n",
        "    # Training\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        outputs = ioc_nn(inputs)  # Forward pass\n",
        "\n",
        "        loss = loss_fun(outputs, targets)  # Calculate the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "\n",
        "        ioc_nn.apply(WeightExponentiation())  # Apply weight exponentiation\n",
        "\n",
        "        current_loss.append(loss.item())  # Append current loss to the list\n",
        "\n",
        "        # Calculate and append accuracy\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item() / targets.shape[0]\n",
        "        current_acc.append(acc)\n",
        "\n",
        "    # Validation\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = ioc_nn(inputs)  # Forward pass for validation\n",
        "\n",
        "        # Calculate and append validation accuracy\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item() / targets.shape[0]\n",
        "        val_acc.append(acc)\n",
        "\n",
        "    # Performance Evaluation\n",
        "    avg_train_loss = sum(current_loss) / len(current_loss)\n",
        "    avg_train_acc = sum(current_acc) / len(current_acc)\n",
        "    avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    print('Loss : %.3f' % avg_train_loss)\n",
        "    print('Training Accuracy : %.3f' % avg_train_acc)\n",
        "    print('Validation Accuracy : %.3f' % avg_val_acc)\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = avg_val_acc - prev_val_acc\n",
        "    print('Update : %.4f' % update)\n",
        "    print(\"-------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if 0 < update < 1e-4:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = avg_val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "fOnpBMcCaYw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []  # List to store test accuracies\n",
        "\n",
        "# Iterate over the test dataset\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    inputs, targets = data\n",
        "    test_outputs = ioc_nn(inputs)\n",
        "    pred = torch.max(test_outputs, 1).indices\n",
        "    acc = (targets == pred).sum().item()\n",
        "    test_acc.append(acc / targets.shape[0])\n",
        "\n",
        "# Calculate average test accuracy\n",
        "avg_test_acc = sum(test_acc) / len(test_acc)\n",
        "print('Test Accuracy: %.3f' % avg_test_acc)"
      ],
      "metadata": {
        "id": "9-4VPcbpj1WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp.2 - Training on Duplicate Free data (ciFAIR10)"
      ],
      "metadata": {
        "id": "qrdvvO0MEKuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets\n",
        "\n",
        "class ciFAIR10(torchvision.datasets.CIFAR10):\n",
        "    base_folder = 'ciFAIR-10'  # Base folder for the dataset\n",
        "    url = 'https://github.com/cvjena/cifair/releases/download/v1.0/ciFAIR-10.zip'  # URL to download the dataset\n",
        "    filename = 'ciFAIR-10.zip'  # Name of the downloaded zip file\n",
        "    tgz_md5 = 'ca08fd390f0839693d3fc45c4e49585f'  # MD5 checksum of the zip file\n",
        "    test_list = [\n",
        "        ['test_batch', '01290e6b622a1977a000eff13650aca2'],  # List of test files along with their MD5 checksums\n",
        "    ]"
      ],
      "metadata": {
        "id": "_q8xsePGEwBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2A_x3y2Ef2U"
      },
      "source": [
        "## NN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "vSHRqysGEf2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469ebd62-f2bc-4d6e-a395-6c8add06e8dc",
        "id": "jUI76_yqEf2V"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/168584397/527c7d80-2645-11e9-8008-a9ca4d2226ec?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231108T134006Z&X-Amz-Expires=300&X-Amz-Signature=5c751b7172c5eb2f471ffd9f151c36078e6f6263662d923363148e2302e1317d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=168584397&response-content-disposition=attachment%3B%20filename%3DciFAIR-10.zip&response-content-type=application%2Foctet-stream to data/ciFAIR-10.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 168614301/168614301 [00:00<00:00, 257535541.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/ciFAIR-10.zip to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])  # Define the transformation pipeline (convert images to tensors)\n",
        "\n",
        "# Load ciFAIR-10 dataset for training and testing\n",
        "train_data = ciFAIR10('data', train=True, download=True, transform=transform)  # Load training data\n",
        "test_data = ciFAIR10('data', train=False, download=True, transform=transform)  # Load test data\n",
        "\n",
        "batch_size = 64  # Set batch size for data loaders\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8  # Ratio of training data\n",
        "validation_ratio = 0.2  # Ratio of validation data\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "# Randomly split train data into train and validation sets\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)  # DataLoader for training data\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)  # DataLoader for validation data\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)  # DataLoader for test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_mNtjBgEf2V"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de80b37-c8dd-45da-aad8-0c0c950d8246",
        "id": "RGP3XTkxEf2V"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss : 2.067\n",
            "Training_Acc  : 0.400\n",
            "Validation_Acc  : 0.444\n",
            "update: 0.4439\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Loss : 1.991\n",
            "Training_Acc  : 0.474\n",
            "Validation_Acc  : 0.466\n",
            "update: 0.0221\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Loss : 1.954\n",
            "Training_Acc  : 0.511\n",
            "Validation_Acc  : 0.477\n",
            "update: 0.0113\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Loss : 1.930\n",
            "Training_Acc  : 0.535\n",
            "Validation_Acc  : 0.494\n",
            "update: 0.0163\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Loss : 1.907\n",
            "Training_Acc  : 0.558\n",
            "Validation_Acc  : 0.505\n",
            "update: 0.0113\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Loss : 1.887\n",
            "Training_Acc  : 0.578\n",
            "Validation_Acc  : 0.503\n",
            "update: -0.0023\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Loss : 1.869\n",
            "Training_Acc  : 0.597\n",
            "Validation_Acc  : 0.510\n",
            "update: 0.0071\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Loss : 1.852\n",
            "Training_Acc  : 0.615\n",
            "Validation_Acc  : 0.512\n",
            "update: 0.0022\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Loss : 1.840\n",
            "Training_Acc  : 0.626\n",
            "Validation_Acc  : 0.516\n",
            "update: 0.0042\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Loss : 1.826\n",
            "Training_Acc  : 0.640\n",
            "Validation_Acc  : 0.521\n",
            "update: 0.0054\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Loss : 1.812\n",
            "Training_Acc  : 0.652\n",
            "Validation_Acc  : 0.516\n",
            "update: -0.0059\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Loss : 1.797\n",
            "Training_Acc  : 0.669\n",
            "Validation_Acc  : 0.528\n",
            "update: 0.0119\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Loss : 1.785\n",
            "Training_Acc  : 0.681\n",
            "Validation_Acc  : 0.518\n",
            "update: -0.0095\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Loss : 1.777\n",
            "Training_Acc  : 0.689\n",
            "Validation_Acc  : 0.530\n",
            "update: 0.0123\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Loss : 1.764\n",
            "Training_Acc  : 0.702\n",
            "Validation_Acc  : 0.533\n",
            "update: 0.0022\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Loss : 1.752\n",
            "Training_Acc  : 0.714\n",
            "Validation_Acc  : 0.528\n",
            "update: -0.0044\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Loss : 1.745\n",
            "Training_Acc  : 0.722\n",
            "Validation_Acc  : 0.538\n",
            "update: 0.0095\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Loss : 1.738\n",
            "Training_Acc  : 0.728\n",
            "Validation_Acc  : 0.536\n",
            "update: -0.0018\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Loss : 1.729\n",
            "Training_Acc  : 0.738\n",
            "Validation_Acc  : 0.541\n",
            "update: 0.0050\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Loss : 1.720\n",
            "Training_Acc  : 0.747\n",
            "Validation_Acc  : 0.532\n",
            "update: -0.0093\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Loss : 1.715\n",
            "Training_Acc  : 0.751\n",
            "Validation_Acc  : 0.534\n",
            "update: 0.0025\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Loss : 1.706\n",
            "Training_Acc  : 0.760\n",
            "Validation_Acc  : 0.537\n",
            "update: 0.0030\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Loss : 1.700\n",
            "Training_Acc  : 0.766\n",
            "Validation_Acc  : 0.532\n",
            "update: -0.0053\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Loss : 1.696\n",
            "Training_Acc  : 0.771\n",
            "Validation_Acc  : 0.539\n",
            "update: 0.0068\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Loss : 1.687\n",
            "Training_Acc  : 0.779\n",
            "Validation_Acc  : 0.541\n",
            "update: 0.0025\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Loss : 1.683\n",
            "Training_Acc  : 0.783\n",
            "Validation_Acc  : 0.535\n",
            "update: -0.0060\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Loss : 1.676\n",
            "Training_Acc  : 0.790\n",
            "Validation_Acc  : 0.540\n",
            "update: 0.0044\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Loss : 1.673\n",
            "Training_Acc  : 0.793\n",
            "Validation_Acc  : 0.537\n",
            "update: -0.0023\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Loss : 1.668\n",
            "Training_Acc  : 0.798\n",
            "Validation_Acc  : 0.539\n",
            "update: 0.0013\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Loss : 1.664\n",
            "Training_Acc  : 0.802\n",
            "Validation_Acc  : 0.536\n",
            "update: -0.0022\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Loss : 1.658\n",
            "Training_Acc  : 0.808\n",
            "Validation_Acc  : 0.534\n",
            "update: -0.0027\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Loss : 1.654\n",
            "Training_Acc  : 0.811\n",
            "Validation_Acc  : 0.542\n",
            "update: 0.0087\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Loss : 1.648\n",
            "Training_Acc  : 0.817\n",
            "Validation_Acc  : 0.536\n",
            "update: -0.0059\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Loss : 1.650\n",
            "Training_Acc  : 0.816\n",
            "Validation_Acc  : 0.531\n",
            "update: -0.0052\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Loss : 1.646\n",
            "Training_Acc  : 0.820\n",
            "Validation_Acc  : 0.543\n",
            "update: 0.0118\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Loss : 1.639\n",
            "Training_Acc  : 0.827\n",
            "Validation_Acc  : 0.542\n",
            "update: -0.0014\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Loss : 1.638\n",
            "Training_Acc  : 0.828\n",
            "Validation_Acc  : 0.544\n",
            "update: 0.0028\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Loss : 1.634\n",
            "Training_Acc  : 0.831\n",
            "Validation_Acc  : 0.535\n",
            "update: -0.0100\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Loss : 1.631\n",
            "Training_Acc  : 0.834\n",
            "Validation_Acc  : 0.537\n",
            "update: 0.0020\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Loss : 1.627\n",
            "Training_Acc  : 0.838\n",
            "Validation_Acc  : 0.533\n",
            "update: -0.0034\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Loss : 1.627\n",
            "Training_Acc  : 0.837\n",
            "Validation_Acc  : 0.533\n",
            "update: -0.0006\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Loss : 1.623\n",
            "Training_Acc  : 0.842\n",
            "Validation_Acc  : 0.536\n",
            "update: 0.0034\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Loss : 1.621\n",
            "Training_Acc  : 0.844\n",
            "Validation_Acc  : 0.542\n",
            "update: 0.0065\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Loss : 1.618\n",
            "Training_Acc  : 0.847\n",
            "Validation_Acc  : 0.534\n",
            "update: -0.0081\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Loss : 1.616\n",
            "Training_Acc  : 0.848\n",
            "Validation_Acc  : 0.539\n",
            "update: 0.0042\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Loss : 1.611\n",
            "Training_Acc  : 0.854\n",
            "Validation_Acc  : 0.541\n",
            "update: 0.0022\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Loss : 1.609\n",
            "Training_Acc  : 0.855\n",
            "Validation_Acc  : 0.535\n",
            "update: -0.0053\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Loss : 1.609\n",
            "Training_Acc  : 0.855\n",
            "Validation_Acc  : 0.535\n",
            "update: -0.0005\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Loss : 1.607\n",
            "Training_Acc  : 0.858\n",
            "Validation_Acc  : 0.527\n",
            "update: -0.0084\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Loss : 1.605\n",
            "Training_Acc  : 0.859\n",
            "Validation_Acc  : 0.543\n",
            "update: 0.0162\n",
            "--------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "mlp = NN()  # Instantiate the MLP model\n",
        "loss_fun = nn.CrossEntropyLoss()  # Define the loss function (Cross Entropy Loss)\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)  # Define the optimizer (Adam optimizer) with a learning rate of 1e-4\n",
        "epoch = 0  # Initialize epoch counter\n",
        "prev_val_acc = 0.0  # Initialize previous validation accuracy\n",
        "tol_epochs = 0  # Initialize tolerance epochs\n",
        "max_epochs = 50  # Maximum number of epochs to train\n",
        "\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1  # Increment epoch counter\n",
        "    print(f'Epoch {epoch}')\n",
        "\n",
        "    current_loss = []  # List to store current epoch's losses\n",
        "    current_acc = []   # List to store current epoch's accuracies\n",
        "    val_acc = []       # List to store validation accuracies\n",
        "\n",
        "    # Training loop\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = mlp(inputs)  # Forward pass\n",
        "\n",
        "        loss = loss_fun(outputs, targets)  # Calculate the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        current_loss.append(loss.item())  # Append current loss\n",
        "\n",
        "        # Calculate and append accuracy\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item() / targets.shape[0]\n",
        "        current_acc.append(acc)\n",
        "\n",
        "    # Validation loop\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = mlp(inputs)  # Forward pass for validation\n",
        "\n",
        "        # Calculate and append validation accuracy\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item() / targets.shape[0]\n",
        "        val_acc.append(acc)\n",
        "\n",
        "    # Print performance metrics\n",
        "    print('Loss: %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "    print('Training Accuracy: %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "    print('Validation Accuracy: %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "    # Early stopping criteria\n",
        "    update = (sum(val_acc) / len(val_acc)) - prev_val_acc\n",
        "    print('Update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if 0 < update < 1e-4:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc) / len(val_acc)  # Update previous validation accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "YbKXFHk6Ef2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9570d9a-d52a-4a6b-98d9-bcdcd005d2ca",
        "id": "kr0jDs6DEf2W"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_Acc  : 0.525\n"
          ]
        }
      ],
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoDsOCKAEf2W"
      },
      "source": [
        "## IOC - NN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "ZCh6VP07Ef2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to perform whitening Transform\n",
        "class WhiteningTransform:\n",
        "    def __init__(self):\n",
        "        self.whitening = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        mean = data.mean(dim=0)\n",
        "        centered_data = data - mean\n",
        "\n",
        "        cov_matrix = torch.mm(centered_data.t(), centered_data) / data.size(0)\n",
        "        U, S, V = torch.svd(cov_matrix)\n",
        "        self.whitening = torch.mm(centered_data, U) / torch.sqrt(S + 1e-10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.whitening is not None:\n",
        "            x = self.whitening.t() @ x.reshape(x.size(0), -1).t()\n",
        "            x = x.t().reshape(x.size(0), 3, 32, 32)\n",
        "            return x\n",
        "        return x\n",
        "\n",
        "# Creating a transform object withwhitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    WhiteningTransform()\n",
        "])\n"
      ],
      "metadata": {
        "id": "Yt39Ds-GEf2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loading the MNIST dataset with whitening transformation\n",
        "train_data = ciFAIR10('data', train=True, download=True, transform=transform)\n",
        "test_data = ciFAIR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c37509-0ede-46e1-85ca-73236eb74f6b",
        "id": "8QswpPd8Ef2X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "odDBiIkjEf2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0193321-e572-49c4-c6fc-0b4408195d27",
        "id": "IzA-fz_SEf2Y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss : 2.262\n",
            "Training_Acc  : 0.162\n",
            "Validation_Acc  : 0.185\n",
            "update: 0.1853\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Loss : 2.234\n",
            "Training_Acc  : 0.206\n",
            "Validation_Acc  : 0.216\n",
            "update: 0.0308\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Loss : 2.222\n",
            "Training_Acc  : 0.221\n",
            "Validation_Acc  : 0.230\n",
            "update: 0.0142\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Loss : 2.214\n",
            "Training_Acc  : 0.230\n",
            "Validation_Acc  : 0.236\n",
            "update: 0.0056\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Loss : 2.207\n",
            "Training_Acc  : 0.239\n",
            "Validation_Acc  : 0.247\n",
            "update: 0.0108\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Loss : 2.199\n",
            "Training_Acc  : 0.250\n",
            "Validation_Acc  : 0.250\n",
            "update: 0.0031\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Loss : 2.194\n",
            "Training_Acc  : 0.253\n",
            "Validation_Acc  : 0.258\n",
            "update: 0.0080\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Loss : 2.190\n",
            "Training_Acc  : 0.259\n",
            "Validation_Acc  : 0.257\n",
            "update: -0.0003\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Loss : 2.188\n",
            "Training_Acc  : 0.261\n",
            "Validation_Acc  : 0.265\n",
            "update: 0.0076\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Loss : 2.186\n",
            "Training_Acc  : 0.264\n",
            "Validation_Acc  : 0.262\n",
            "update: -0.0026\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Loss : 2.183\n",
            "Training_Acc  : 0.266\n",
            "Validation_Acc  : 0.267\n",
            "update: 0.0050\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Loss : 2.181\n",
            "Training_Acc  : 0.268\n",
            "Validation_Acc  : 0.270\n",
            "update: 0.0024\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Loss : 2.180\n",
            "Training_Acc  : 0.269\n",
            "Validation_Acc  : 0.269\n",
            "update: -0.0005\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Loss : 2.177\n",
            "Training_Acc  : 0.272\n",
            "Validation_Acc  : 0.270\n",
            "update: 0.0003\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Loss : 2.176\n",
            "Training_Acc  : 0.274\n",
            "Validation_Acc  : 0.271\n",
            "update: 0.0014\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Loss : 2.175\n",
            "Training_Acc  : 0.275\n",
            "Validation_Acc  : 0.274\n",
            "update: 0.0029\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Loss : 2.174\n",
            "Training_Acc  : 0.275\n",
            "Validation_Acc  : 0.273\n",
            "update: -0.0005\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Loss : 2.173\n",
            "Training_Acc  : 0.278\n",
            "Validation_Acc  : 0.276\n",
            "update: 0.0031\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Loss : 2.172\n",
            "Training_Acc  : 0.279\n",
            "Validation_Acc  : 0.280\n",
            "update: 0.0033\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Loss : 2.170\n",
            "Training_Acc  : 0.281\n",
            "Validation_Acc  : 0.283\n",
            "update: 0.0032\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Loss : 2.169\n",
            "Training_Acc  : 0.281\n",
            "Validation_Acc  : 0.279\n",
            "update: -0.0042\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Loss : 2.167\n",
            "Training_Acc  : 0.285\n",
            "Validation_Acc  : 0.283\n",
            "update: 0.0046\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Loss : 2.167\n",
            "Training_Acc  : 0.284\n",
            "Validation_Acc  : 0.283\n",
            "update: -0.0001\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Loss : 2.166\n",
            "Training_Acc  : 0.286\n",
            "Validation_Acc  : 0.284\n",
            "update: 0.0009\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Loss : 2.164\n",
            "Training_Acc  : 0.288\n",
            "Validation_Acc  : 0.287\n",
            "update: 0.0033\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Loss : 2.163\n",
            "Training_Acc  : 0.290\n",
            "Validation_Acc  : 0.287\n",
            "update: -0.0008\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Loss : 2.162\n",
            "Training_Acc  : 0.290\n",
            "Validation_Acc  : 0.287\n",
            "update: 0.0005\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Loss : 2.160\n",
            "Training_Acc  : 0.294\n",
            "Validation_Acc  : 0.287\n",
            "update: -0.0001\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Loss : 2.160\n",
            "Training_Acc  : 0.293\n",
            "Validation_Acc  : 0.293\n",
            "update: 0.0057\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Loss : 2.157\n",
            "Training_Acc  : 0.297\n",
            "Validation_Acc  : 0.292\n",
            "update: -0.0005\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Loss : 2.156\n",
            "Training_Acc  : 0.299\n",
            "Validation_Acc  : 0.295\n",
            "update: 0.0024\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Loss : 2.154\n",
            "Training_Acc  : 0.300\n",
            "Validation_Acc  : 0.298\n",
            "update: 0.0033\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Loss : 2.153\n",
            "Training_Acc  : 0.302\n",
            "Validation_Acc  : 0.297\n",
            "update: -0.0012\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Loss : 2.152\n",
            "Training_Acc  : 0.304\n",
            "Validation_Acc  : 0.302\n",
            "update: 0.0057\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Loss : 2.150\n",
            "Training_Acc  : 0.305\n",
            "Validation_Acc  : 0.300\n",
            "update: -0.0024\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Loss : 2.150\n",
            "Training_Acc  : 0.305\n",
            "Validation_Acc  : 0.303\n",
            "update: 0.0031\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Loss : 2.148\n",
            "Training_Acc  : 0.307\n",
            "Validation_Acc  : 0.305\n",
            "update: 0.0018\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Loss : 2.148\n",
            "Training_Acc  : 0.306\n",
            "Validation_Acc  : 0.303\n",
            "update: -0.0019\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Loss : 2.148\n",
            "Training_Acc  : 0.306\n",
            "Validation_Acc  : 0.303\n",
            "update: -0.0002\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Loss : 2.147\n",
            "Training_Acc  : 0.308\n",
            "Validation_Acc  : 0.305\n",
            "update: 0.0025\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Loss : 2.146\n",
            "Training_Acc  : 0.309\n",
            "Validation_Acc  : 0.306\n",
            "update: 0.0004\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Loss : 2.145\n",
            "Training_Acc  : 0.311\n",
            "Validation_Acc  : 0.301\n",
            "update: -0.0042\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Loss : 2.145\n",
            "Training_Acc  : 0.311\n",
            "Validation_Acc  : 0.306\n",
            "update: 0.0046\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Loss : 2.144\n",
            "Training_Acc  : 0.310\n",
            "Validation_Acc  : 0.303\n",
            "update: -0.0029\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Loss : 2.144\n",
            "Training_Acc  : 0.312\n",
            "Validation_Acc  : 0.302\n",
            "update: -0.0007\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Loss : 2.143\n",
            "Training_Acc  : 0.311\n",
            "Validation_Acc  : 0.307\n",
            "update: 0.0043\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Loss : 2.142\n",
            "Training_Acc  : 0.314\n",
            "Validation_Acc  : 0.308\n",
            "update: 0.0009\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Loss : 2.143\n",
            "Training_Acc  : 0.313\n",
            "Validation_Acc  : 0.306\n",
            "update: -0.0015\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Loss : 2.143\n",
            "Training_Acc  : 0.311\n",
            "Validation_Acc  : 0.306\n",
            "update: -0.0006\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Loss : 2.142\n",
            "Training_Acc  : 0.314\n",
            "Validation_Acc  : 0.308\n",
            "update: 0.0021\n",
            "-------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "ioc_nn = IOC_NN()\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "# Slowing down the learning rate decay using beta values\n",
        "optimizer = torch.optim.Adam(ioc_nn.parameters(), lr=1e-4,betas=(0.9,0.9))\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 50\n",
        "\n",
        "\n",
        "while(epoch <max_epochs and tol_epochs<2):\n",
        "    epoch+=1\n",
        "    print(f'Epoch {epoch}')\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    tolerance = 1e-4\n",
        "    val_acc = []\n",
        "    # Training\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "      inputs, targets = data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = ioc_nn(inputs)\n",
        "\n",
        "\n",
        "      loss = loss_fun(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      ioc_nn.apply(WeightExponentiation())\n",
        "\n",
        "      current_loss.append(loss.item())\n",
        "\n",
        "      pred = torch.max(outputs,1).indices\n",
        "      acc= (targets == pred).sum().item()\n",
        "      current_acc.append(acc/targets.shape[0])\n",
        "\n",
        "    # Validation\n",
        "    for i,data in enumerate(val_loader,0):\n",
        "      inputs,targets = data\n",
        "      val_outputs = ioc_nn(inputs)\n",
        "      pred = torch.max(val_outputs,1).indices\n",
        "      acc =(targets==pred).sum().item()\n",
        "      val_acc.append(acc/targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "\n",
        "    print('Loss : %.3f' %(sum(current_loss) /len(current_loss)))\n",
        "    print('Training_Acc  : %.3f'%(sum(current_acc)/len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f'%(sum(val_acc)/len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"-------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if(update>0 and update<tolerance):\n",
        "      tol_epochs+=1\n",
        "    else:\n",
        "      tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc)/len(val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "Gxy8g82eEf2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = ioc_nn(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-nq_BwSEf2Z",
        "outputId": "db1f1d54-1463-4633-e18d-743f0aac42a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_Acc  : 0.315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp.3 - Ensembles of Binary Experts"
      ],
      "metadata": {
        "id": "HYTSNVpxjiHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **BinaryExpert Model:** This model represents a single binary classification expert. It consists of several fully connected layers followed by batch normalization and ELU activation functions. The output layer consists of a single unit followed by a sigmoid activation function, which outputs the probability of the positive class.\n",
        "\n",
        "2. **GatingNetwork Model:** This model is responsible for predicting the weights for each expert in the ensemble. It takes the flattened input tensor and passes it through a fully connected layer. The output of this layer is then passed through a softmax activation function to obtain the weights for each expert.\n",
        "\n",
        "3. **Ensemble of Binary Experts:** To create an ensemble of binary experts, multiple instances of the BinaryExpert model are instantiated. Additionally, a single instance of the GatingNetwork model is instantiated to compute the weights for each expert. During training, the input data is passed through the gating network to obtain the weights for each expert. Then, each expert model is individually fed the input data, and their predictions are weighted by the corresponding weights obtained from the gating network. Finally, the weighted predictions are combined to obtain the ensemble prediction."
      ],
      "metadata": {
        "id": "5TiJ9YNMc4Us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC-NN Model"
      ],
      "metadata": {
        "id": "WC5RyoYv4ERo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to perform whitening Transform\n",
        "class WhiteningTransform:\n",
        "    def __init__(self):\n",
        "        self.whitening = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        mean = data.mean(dim=0)\n",
        "        centered_data = data - mean\n",
        "\n",
        "        cov_matrix = torch.mm(centered_data.t(), centered_data) / data.size(0)\n",
        "        U, S, V = torch.svd(cov_matrix)\n",
        "        self.whitening = torch.mm(centered_data, U) / torch.sqrt(S + 1e-10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.whitening is not None:\n",
        "            x = self.whitening.t() @ x.reshape(x.size(0), -1).t()\n",
        "            x = x.t().reshape(x.size(0), 3, 32, 32)\n",
        "            return x\n",
        "        return x\n",
        "\n",
        "# Creating a transform object withwhitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    WhiteningTransform()\n",
        "])"
      ],
      "metadata": {
        "id": "a87df-iwSZX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Converting the original data to Binary \"1\" for Animal and \"0\" for Not an Animal\n",
        "train_data.targets = torch.FloatTensor([1 if i>=2 and i<=7 else 0 for i in train_data.targets])\n",
        "test_data.targets = torch.FloatTensor([1 if i>=2 and i<=7 else 0 for i in test_data.targets])\n",
        "\n",
        "batch_size = 64\n",
        "# Spliting train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for all the train,val and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s7jOAGQngTq",
        "outputId": "ae67f26d-c76b-4630-8e25-d97be7badf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 28526723.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryExpert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the layers of the binary expert model\n",
        "        self.first_hidden_layer = nn.Linear(32*32*3, 1000)  # Input layer to first hidden layer\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),           # Flatten the input\n",
        "            self.first_hidden_layer,  # First hidden layer\n",
        "            nn.BatchNorm1d(1000),   # Batch normalization\n",
        "            nn.ELU(True),           # ELU activation function\n",
        "\n",
        "            nn.Linear(1000, 800),   # Second hidden layer\n",
        "            nn.BatchNorm1d(800),    # Batch normalization\n",
        "            nn.ELU(True),           # ELU activation function\n",
        "\n",
        "            nn.Linear(800, 800),    # Third hidden layer\n",
        "            nn.BatchNorm1d(800),    # Batch normalization\n",
        "            nn.ELU(True),           # ELU activation function\n",
        "\n",
        "            nn.Linear(800, 1),      # Output layer (single unit for binary classification)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation function for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the layers\n",
        "        x = self.layers(x)\n",
        "        x = self.sigmoid(x)  # Apply sigmoid activation to get probabilities\n",
        "        return x"
      ],
      "metadata": {
        "id": "UZNCPXRpunBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingNetwork(nn.Module):\n",
        "    def __init__(self, input_channels, input_height, input_width, num_experts):\n",
        "        super().__init__()\n",
        "        input_size = input_channels * input_height * input_width  # Calculate the input size\n",
        "        self.fc = nn.Linear(input_size, num_experts)  # Fully connected layer to predict expert weights\n",
        "        self.softmax = nn.Softmax(dim=1)  # Softmax activation function along the dimension of experts\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.size(0), -1)  # Flatten the input tensor\n",
        "        x = self.softmax(self.fc(x))  # Pass through fully connected layer and apply softmax\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "tigbyHc6ustr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Weight Exponentiation\n",
        "class WeightExponentiation(object):\n",
        "\n",
        "    def __init__(self, epsilon = 5):\n",
        "         # epsilon for constraining exponentiation of weights\n",
        "         self.epsilon = epsilon\n",
        "\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if hasattr(module, 'weight'):\n",
        "            # Selecting all other layers except \"first_hidden_layer\"\n",
        "            if \"first_hidden_layer\" not in module.__str__():\n",
        "                w = module.weight.data\n",
        "                # Perform exponentiation operation on negative weights\n",
        "                w[w < 0] = torch.exp(w[w < 0]-self.epsilon)\n",
        "                module.weight.data = w # Storing the updated weights"
      ],
      "metadata": {
        "id": "L4AiKr-C1ujt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_channels = 3\n",
        "input_height = 32\n",
        "input_width = 32\n",
        "num_experts = 4\n",
        "\n",
        "# Instantiate the gating network\n",
        "gating_net = GatingNetwork(input_channels, input_height, input_width, num_experts)\n",
        "\n",
        "# Instantiate expert networks and store them in a ModuleList\n",
        "expert_nets = nn.ModuleList([BinaryExpert() for _ in range(num_experts)])\n",
        "\n",
        "# Combine parameters for optimization\n",
        "parameters = list(gating_net.parameters())  # Get parameters of the gating network\n",
        "for expert_net in expert_nets:\n",
        "    parameters += list(expert_net.parameters())  # Get parameters of each expert network\n"
      ],
      "metadata": {
        "id": "4zu3FBN6Fmuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "optimizer = optim.Adam(parameters, lr=0.0001, betas=(0.9, 0.9))\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    for data, labels in train_loader:\n",
        "        # E-step: Obtain gating coefficients from the gating network\n",
        "        gating_coefficients = gating_net(data).float()\n",
        "\n",
        "        # M-step: Update gating network parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(gating_coefficients[:, 1], labels.float())  # Use BCELoss on the second coefficient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Apply weight exponentiation to expert networks\n",
        "        for expert_net in expert_nets:\n",
        "            expert_net.apply(WeightExponentiation())\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        for data, labels in val_loader:\n",
        "            gating_coefficients = gating_net(data)\n",
        "            val_loss += criterion(gating_coefficients[:, 1], labels)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
        "\n",
        "# Test the model on the test set\n",
        "with torch.no_grad():\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, labels in test_loader:\n",
        "        gating_coefficients = gating_net(data)\n",
        "        test_loss += criterion(gating_coefficients[:, 1], labels)\n",
        "\n",
        "        predicted_labels = (gating_coefficients[:, 1] > 0.5).float()  # Threshold gating coefficients for binary prediction\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVT6A0CXCHgg",
        "outputId": "041bb200-07a9-4ceb-eb24-b35bc2e2142d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 0.40685030817985535, Validation Loss: 0.4584801197052002\n",
            "Epoch 2/10, Training Loss: 0.4153115451335907, Validation Loss: 0.4627629220485687\n",
            "Epoch 3/10, Training Loss: 0.44500815868377686, Validation Loss: 0.44609904289245605\n",
            "Epoch 4/10, Training Loss: 0.5660567879676819, Validation Loss: 0.4430065155029297\n",
            "Epoch 5/10, Training Loss: 0.45420771837234497, Validation Loss: 0.4399407207965851\n",
            "Epoch 6/10, Training Loss: 0.4201667010784149, Validation Loss: 0.4388028681278229\n",
            "Epoch 7/10, Training Loss: 0.3349223732948303, Validation Loss: 0.4423254728317261\n",
            "Epoch 8/10, Training Loss: 0.3934375047683716, Validation Loss: 0.43683770298957825\n",
            "Epoch 9/10, Training Loss: 0.4281715154647827, Validation Loss: 0.4289214611053467\n",
            "Epoch 10/10, Training Loss: 0.4822363257408142, Validation Loss: 0.4294072985649109\n",
            "Test Loss: 0.4208977520465851, Test Accuracy: 0.8183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp.4 - BoostedEnsemble"
      ],
      "metadata": {
        "id": "_EDEHp3vE4On"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Initialization**:\n",
        "  - The class `ExpertEnsemble` is initialized with parameters such as `base_learner` representing the base model architecture and `num_experts` indicating the number of experts in the ensemble.\n",
        "  - `self.experts` is created as a list containing `num_experts` experts, each instantiated using the provided base learner function.\n",
        "  - `self.criterion` is set to the cross-entropy loss function for training.\n",
        "\n",
        "2. **Training Method**:\n",
        "  - `train_experts` method trains each expert individually using bootstrapped samples and updated weights.\n",
        "  - Sample weights for the entire training dataset are initialized, and for each expert:\n",
        "    - The expert is trained for a fixed number of epochs using bootstrapped samples.\n",
        "    - Sample weights are updated based on the expert's performance, and a new dataloader with bootstrapped samples is created for the next expert.\n",
        "\n",
        "3. **Training Single Expert Method**:\n",
        "  - `train_single_expert` method trains a single expert for a fixed number of epochs using the Adam optimizer.\n",
        "  - Loss is calculated using the cross-entropy loss function, and model parameters are updated.\n",
        "  - Weight exponentiation is applied after each update to maintain diversity among experts.\n",
        "\n",
        "4. **Sample Weight Update Method**:\n",
        "  - `update_sample_weights` method updates sample weights based on misclassifications of a weak learner (expert).\n",
        "  - Cross-entropy loss and gradients are used to adjust sample weights, and normalization is applied to ensure proper weighting."
      ],
      "metadata": {
        "id": "Uxm712Zidn8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC-NN Model"
      ],
      "metadata": {
        "id": "yPCDSaI-4Rym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:37.355763Z",
          "iopub.execute_input": "2023-11-20T13:41:37.356169Z",
          "iopub.status.idle": "2023-11-20T13:41:37.430713Z",
          "shell.execute_reply.started": "2023-11-20T13:41:37.356143Z",
          "shell.execute_reply": "2023-11-20T13:41:37.429756Z"
        },
        "trusted": true,
        "id": "CQnCML6wKZGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a transform object with whitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "   WhiteningTransform()\n",
        "])\n",
        "# Loading the MNIST dataset with whitening transformation\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "_sCY8Yr-5X25",
        "outputId": "378a2da0-ce7c-450a-8d1e-6a0b5cd32816",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:37.454863Z",
          "iopub.execute_input": "2023-11-20T13:41:37.455215Z",
          "iopub.status.idle": "2023-11-20T13:41:44.921383Z",
          "shell.execute_reply.started": "2023-11-20T13:41:37.455167Z",
          "shell.execute_reply": "2023-11-20T13:41:44.920401Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 170498071/170498071 [00:03<00:00, 43428993.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class ExpertEnsemble(nn.Module):\n",
        "    def __init__(self, base_learner, num_experts: int = 3):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        # Initialize a list of experts with the provided base learner\n",
        "        self.experts = nn.ModuleList([base_learner().to(device) for _ in range(self.num_experts)])\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input data through each expert and return their outputs\n",
        "        expert_outputs = [expert(x) for expert in self.experts]\n",
        "        return expert_outputs\n",
        "\n",
        "    def train_experts(self, train_loader):\n",
        "        # Initialize sample weights for training data\n",
        "        weights = torch.ones(len(train_loader.dataset))\n",
        "        for i in range(self.num_experts):\n",
        "            print(\"Training Expert:\", i)\n",
        "            # Train a single expert with updated weights\n",
        "            self.train_single_expert(self.experts[i], train_loader)\n",
        "            # Update sample weights based on expert's performance\n",
        "            weights = self.update_sample_weights(self.experts[i], train_loader.dataset, weights)\n",
        "            # Create a new DataLoader with bootstrapped samples based on updated weights\n",
        "            train_loader = self.bootstrap_dataloader(train_loader, weights)\n",
        "\n",
        "    def train_single_expert(self, model, train_loader):\n",
        "        model.train()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=[0.9, 0.9])\n",
        "        max_epochs = 2\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            print(f'Epoch {epoch + 1}')\n",
        "            current_loss = []\n",
        "            current_acc = []\n",
        "\n",
        "            # Training loop\n",
        "            for i, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Apply weight exponentiation after each update\n",
        "                model.apply(WeightExponentiation())\n",
        "\n",
        "                current_loss.append(loss.item())\n",
        "                pred = torch.max(outputs, 1).indices\n",
        "                acc = (targets == pred).sum().item()\n",
        "                current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "            # Performance evaluation\n",
        "            print('Loss: %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "            print('Training Accuracy: %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "\n",
        "    def update_sample_weights(self, weak_learner, dataset, sample_weights, learning_rate=0.001):\n",
        "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = weak_learner(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update sample weights based on misclassifications\n",
        "            misclassifications = (outputs.argmax(dim=1) != labels).float()\n",
        "            sample_weights *= torch.exp(learning_rate * misclassifications)\n",
        "            # Normalize weights\n",
        "            sample_weights /= sample_weights.sum()\n",
        "\n",
        "        return sample_weights\n",
        "\n",
        "    def bootstrap_dataloader(self, dataloader, sample_weights):\n",
        "        num_samples = len(dataloader.dataset)\n",
        "        bootstrap_indices = torch.multinomial(sample_weights, num_samples, replacement=True).int()\n",
        "        # Create a new DataLoader with bootstrapped samples\n",
        "        return DataLoader([dataloader.dataset[i] for i in bootstrap_indices])"
      ],
      "metadata": {
        "id": "cRK9EDGwOeQw",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.953986Z",
          "iopub.execute_input": "2023-11-20T13:41:44.954334Z",
          "iopub.status.idle": "2023-11-20T13:41:44.974792Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.954301Z",
          "shell.execute_reply": "2023-11-20T13:41:44.973736Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoostingEnsemble:\n",
        "    def __init__(self, num_experts, base_learner, gating_network, train_loader):\n",
        "        self.num_experts = num_experts\n",
        "        self.base_learner = base_learner\n",
        "        self.gating_network = gating_network(num_experts).to(device)\n",
        "        self.train_loader = train_loader\n",
        "\n",
        "        self.expert_training()\n",
        "        self.gn_training()\n",
        "        self.testing()\n",
        "\n",
        "    def expert_training(self):\n",
        "        # Train the individual experts\n",
        "        model = ExpertEnsemble(self.base_learner, self.num_experts)\n",
        "        model.train_experts(self.train_loader)\n",
        "        print(\"Expert Training Completed\")\n",
        "        self.trained_experts = model.experts\n",
        "\n",
        "        # Enable gradient calculation for experts' parameters\n",
        "        for expert in self.trained_experts:\n",
        "            for param in expert.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def gn_training(self):\n",
        "        optimizer = torch.optim.Adam(self.gating_network.parameters(), lr=1e-4, betas=[0.9, 0.9])\n",
        "        max_epochs = 5\n",
        "        loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            print(f'Epoch {epoch + 1}')\n",
        "            current_loss = []\n",
        "            current_acc = []\n",
        "\n",
        "            # Training loop\n",
        "            for i, (inputs, targets) in enumerate(self.train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass through experts and concatenation\n",
        "                combined_outputs = torch.cat([expert(inputs) for expert in self.trained_experts], dim=1)\n",
        "\n",
        "                outputs = self.gating_network(combined_outputs)\n",
        "                loss = loss_fun(outputs, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Apply weight exponentiation after each update\n",
        "                self.gating_network.apply(WeightExponentiation())\n",
        "                current_loss.append(loss.item())\n",
        "\n",
        "                pred = torch.max(outputs, 1).indices\n",
        "                acc = (targets == pred).sum().item()\n",
        "                current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "            # Performance Evaluation\n",
        "            print('Loss: %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "            print('Training Accuracy: %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "\n",
        "        print(\"Training of Gating Network Completed\")\n",
        "\n",
        "    def testing(self):\n",
        "        test_acc = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                # Forward pass through experts and concatenation\n",
        "                combined_outputs = torch.cat([expert(inputs) for expert in self.trained_experts], dim=1)\n",
        "                test_outputs = self.gating_network(combined_outputs)\n",
        "\n",
        "                pred = torch.max(test_outputs, 1).indices\n",
        "                acc = (targets == pred).sum().item()\n",
        "                test_acc.append(acc / targets.shape[0])\n",
        "\n",
        "        print('Test Accuracy: %.3f' % (sum(test_acc) / len(test_acc)))"
      ],
      "metadata": {
        "id": "Bd1pC0D55N3o",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.976007Z",
          "iopub.execute_input": "2023-11-20T13:41:44.976280Z",
          "iopub.status.idle": "2023-11-20T13:41:45.000598Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.976256Z",
          "shell.execute_reply": "2023-11-20T13:41:44.999605Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NN_Gating(nn.Module):\n",
        "    def __init__(self, n_experts: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the Gating Network for the IOC-NN ensemble.\n",
        "\n",
        "        Args:\n",
        "            n_experts (int): Number of experts in the ensemble.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_experts = n_experts\n",
        "\n",
        "        # Define the first hidden layer with an input size calculated based on the number of experts\n",
        "        self.first_hidden_layer = nn.Linear(self.n_experts * 10, 1000)\n",
        "\n",
        "        # Define the sequential layers for the gating network\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            self.first_hidden_layer,\n",
        "            nn.BatchNorm1d(1000),\n",
        "            nn.ELU(True),\n",
        "            nn.Linear(1000, 800),\n",
        "            nn.BatchNorm1d(800),\n",
        "            nn.ELU(True),\n",
        "            nn.Linear(800, 10),  # Output Layer\n",
        "            nn.Softmax(1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the gating network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor representing concatenated outputs from the ensemble.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor representing class probabilities.\n",
        "        \"\"\"\n",
        "        # Reshape the input tensor to have a suitable shape for the network\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "\n",
        "        # Pass the input through the defined layers\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4vpipBZTcBk_",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:21:56.316681Z",
          "iopub.execute_input": "2023-11-20T13:21:56.316964Z",
          "iopub.status.idle": "2023-11-20T13:21:56.326771Z",
          "shell.execute_reply.started": "2023-11-20T13:21:56.316942Z",
          "shell.execute_reply": "2023-11-20T13:21:56.325943Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Boosting_Ensemble(3,IOC_NN,NN_Gating,train_loader)"
      ],
      "metadata": {
        "id": "6WovG3K3_200",
        "outputId": "86a96423-1a2e-4583-827e-8f11bc9db919",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:21:56.327928Z",
          "iopub.execute_input": "2023-11-20T13:21:56.328207Z",
          "iopub.status.idle": "2023-11-20T13:24:14.803878Z",
          "shell.execute_reply.started": "2023-11-20T13:21:56.328185Z",
          "shell.execute_reply": "2023-11-20T13:24:14.802958Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Expert: 0\nEpoch 1\nLoss: 2.258\nTraining Accuracy: 0.172\nEpoch 2\nLoss: 2.227\nTraining Accuracy: 0.215\nExpert: 1\nEpoch 1\nLoss: 2.256\nTraining Accuracy: 0.174\nEpoch 2\nLoss: 2.226\nTraining Accuracy: 0.217\nExpert: 2\nEpoch 1\nLoss: 2.255\nTraining Accuracy: 0.172\nEpoch 2\nLoss: 2.226\nTraining Accuracy: 0.216\nExpert Training Completed\nEpoch 1\nLoss : 2.215\nTraining_Acc  : 0.231\nEpoch 2\nLoss : 2.204\nTraining_Acc  : 0.242\nEpoch 3\nLoss : 2.200\nTraining_Acc  : 0.247\nEpoch 4\nLoss : 2.198\nTraining_Acc  : 0.247\nEpoch 5\nLoss : 2.195\nTraining_Acc  : 0.253\nTraining of Gating Network Completed\nTest_Acc  : 0.261\n",
          "output_type": "stream"
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<__main__.Boosting_Ensemble at 0x7cb59ec45030>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp.5 - MNIST"
      ],
      "metadata": {
        "id": "nOgorPehY68F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBmsipScpz0p"
      },
      "source": [
        "## NN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "1ULO_ESupz0p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyhcuNZEpz0o"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTVAeLNGpz0p"
      },
      "outputs": [],
      "source": [
        "# Loading Trian and Test data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = MNIST('data', train=True, download=True, transform=transform)\n",
        "test_data = MNIST('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "# Spliting train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for all the train,val and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "oE5PSrJhpz0p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zmSMAXmpz0p"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(28*28*1,800), # Hidden Layer 1\n",
        "        nn.BatchNorm1d(800),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(800,800), # Hidden Layer 2\n",
        "        nn.BatchNorm1d(800),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(800,800), # Hidden Layer 3\n",
        "        nn.BatchNorm1d(800),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(800,10), # Output Layer\n",
        "        nn.Softmax(1)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37XzbOqpz0q"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67f0a11-82de-40ad-dc77-9944bf7c68fa",
        "id": "fao1xSWepz0q"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss : 1.564\n",
            "Training_Acc  : 0.924\n",
            "Validation_Acc  : 0.959\n",
            "update: 0.9594\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Loss : 1.497\n",
            "Training_Acc  : 0.971\n",
            "Validation_Acc  : 0.968\n",
            "update: 0.0081\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Loss : 1.486\n",
            "Training_Acc  : 0.980\n",
            "Validation_Acc  : 0.974\n",
            "update: 0.0061\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Loss : 1.482\n",
            "Training_Acc  : 0.983\n",
            "Validation_Acc  : 0.973\n",
            "update: -0.0006\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Loss : 1.477\n",
            "Training_Acc  : 0.987\n",
            "Validation_Acc  : 0.976\n",
            "update: 0.0025\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Loss : 1.475\n",
            "Training_Acc  : 0.989\n",
            "Validation_Acc  : 0.973\n",
            "update: -0.0030\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Loss : 1.473\n",
            "Training_Acc  : 0.990\n",
            "Validation_Acc  : 0.972\n",
            "update: -0.0002\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Loss : 1.472\n",
            "Training_Acc  : 0.991\n",
            "Validation_Acc  : 0.976\n",
            "update: 0.0035\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Loss : 1.471\n",
            "Training_Acc  : 0.991\n",
            "Validation_Acc  : 0.973\n",
            "update: -0.0030\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Loss : 1.471\n",
            "Training_Acc  : 0.992\n",
            "Validation_Acc  : 0.979\n",
            "update: 0.0057\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Loss : 1.469\n",
            "Training_Acc  : 0.994\n",
            "Validation_Acc  : 0.976\n",
            "update: -0.0027\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Loss : 1.469\n",
            "Training_Acc  : 0.993\n",
            "Validation_Acc  : 0.977\n",
            "update: 0.0007\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Loss : 1.468\n",
            "Training_Acc  : 0.994\n",
            "Validation_Acc  : 0.977\n",
            "update: 0.0000\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Loss : 1.468\n",
            "Training_Acc  : 0.994\n",
            "Validation_Acc  : 0.976\n",
            "update: -0.0007\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Loss : 1.468\n",
            "Training_Acc  : 0.994\n",
            "Validation_Acc  : 0.977\n",
            "update: 0.0014\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Loss : 1.467\n",
            "Training_Acc  : 0.995\n",
            "Validation_Acc  : 0.977\n",
            "update: 0.0002\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Loss : 1.467\n",
            "Training_Acc  : 0.995\n",
            "Validation_Acc  : 0.978\n",
            "update: 0.0006\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Loss : 1.466\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.977\n",
            "update: -0.0009\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Loss : 1.466\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.981\n",
            "update: 0.0036\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Loss : 1.466\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.979\n",
            "update: -0.0015\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Loss : 1.466\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.979\n",
            "update: -0.0001\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Loss : 1.466\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.977\n",
            "update: -0.0020\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Loss : 1.466\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.980\n",
            "update: 0.0033\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Loss : 1.466\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.979\n",
            "update: -0.0016\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.981\n",
            "update: 0.0024\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.977\n",
            "update: -0.0042\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.996\n",
            "Validation_Acc  : 0.981\n",
            "update: 0.0043\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.978\n",
            "update: -0.0029\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.982\n",
            "update: 0.0031\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.980\n",
            "update: -0.0012\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.980\n",
            "update: 0.0002\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.980\n",
            "update: -0.0002\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.981\n",
            "update: 0.0005\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.980\n",
            "update: -0.0008\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Loss : 1.465\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.980\n",
            "update: -0.0002\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.980\n",
            "update: 0.0004\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.980\n",
            "update: -0.0002\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.980\n",
            "update: -0.0002\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.981\n",
            "update: 0.0012\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.981\n",
            "update: 0.0007\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.980\n",
            "update: -0.0017\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.979\n",
            "update: -0.0006\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.982\n",
            "update: 0.0030\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.980\n",
            "update: -0.0024\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.980\n",
            "update: 0.0001\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.997\n",
            "Validation_Acc  : 0.980\n",
            "update: 0.0000\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.979\n",
            "update: -0.0007\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.982\n",
            "update: 0.0029\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.981\n",
            "update: -0.0007\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Loss : 1.464\n",
            "Training_Acc  : 0.998\n",
            "Validation_Acc  : 0.981\n",
            "update: -0.0008\n",
            "--------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "mlp = NN()\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 50\n",
        "\n",
        "while(epoch <max_epochs and tol_epochs<2):\n",
        "    epoch+=1\n",
        "    print(f'Epoch {epoch}')\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    tolerance = 1e-4\n",
        "    val_acc = []\n",
        "\n",
        "    # Training\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "      inputs, targets = data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "\n",
        "      loss = loss_fun(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss.append(loss.item())\n",
        "\n",
        "      pred = torch.max(outputs,1).indices\n",
        "      acc= (targets == pred).sum().item()\n",
        "      current_acc.append(acc/targets.shape[0])\n",
        "\n",
        "    # Validation\n",
        "    for i,data in enumerate(val_loader,0):\n",
        "      inputs,targets = data\n",
        "      val_outputs = mlp(inputs)\n",
        "      pred = torch.max(val_outputs,1).indices\n",
        "      acc =(targets==pred).sum().item()\n",
        "      val_acc.append(acc/targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "\n",
        "    print('Loss : %.3f' %(sum(current_loss) /len(current_loss)))\n",
        "    print('Training_Acc  : %.3f'%(sum(current_acc)/len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f'%(sum(val_acc)/len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if(update>0 and update<tolerance):\n",
        "      tol_epochs+=1\n",
        "    else:\n",
        "      tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc)/len(val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "n1wftvXnpz0q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a22f339-5d3f-4e02-be4e-f03e537ce018",
        "id": "0qXhHNvDpz0q"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_Acc  : 0.981\n"
          ]
        }
      ],
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg7rmAU3pz0q"
      },
      "source": [
        "## IOC - NN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "QfMqpXDcpz0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to perform whitening Transform\n",
        "class WhiteningTransform:\n",
        "    def __init__(self):\n",
        "        self.whitening = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        mean = data.mean(dim=0)\n",
        "        centered_data = data - mean\n",
        "\n",
        "        cov_matrix = torch.mm(centered_data.t(), centered_data) / data.size(0)\n",
        "        U, S, V = torch.svd(cov_matrix)\n",
        "        self.whitening = torch.mm(centered_data, U) / torch.sqrt(S + 1e-10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.whitening is not None:\n",
        "            x = self.whitening.t() @ x.reshape(x.size(0), -1).t()\n",
        "            x = x.t().reshape(x.size(0), 1, 28, 28)\n",
        "            return x\n",
        "        return x\n",
        "\n",
        "# Creating a transform object withwhitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    WhiteningTransform()\n",
        "])\n"
      ],
      "metadata": {
        "id": "pZYLtcFwpz0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loading the MNIST dataset with whitening transformation\n",
        "train_data = MNIST('data', train=True, download=True, transform=transform)\n",
        "test_data = MNIST('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f601ea92-9ec7-4e94-b5f2-12c08996f5d7",
        "id": "I6k_a3grpz0r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 148088546.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 45318254.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 35157445.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14185054.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "YgbVV8Fkpz0r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHk5U6Xcpz0r"
      },
      "outputs": [],
      "source": [
        "class IOC_NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Increasing the no of nodes of the first hidden layer that allows negative weights\n",
        "    self.first_hidden_layer = nn.Linear(28*28*1,1000) # Hidden Layer 1\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        self.first_hidden_layer,\n",
        "        nn.BatchNorm1d(1000),\n",
        "        nn.ELU(True),\n",
        "\n",
        "        nn.Linear(1000,800), # Hidden Layer 2\n",
        "        nn.BatchNorm1d(800),\n",
        "        nn.ELU(True),\n",
        "\n",
        "        nn.Linear(800,800), # Hidden Layer 3\n",
        "        nn.BatchNorm1d(800),\n",
        "        nn.ELU(True),\n",
        "\n",
        "        nn.Linear(800,10), # Output Layer\n",
        "        nn.Softmax(1)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Weight Exponentiation\n",
        "class WeightExponentiation(object):\n",
        "\n",
        "    def __init__(self, epsilon = 5):\n",
        "        self.epsilon = epsilon # epsilon for constraining exponentiation of weights\n",
        "\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if hasattr(module, 'weight'):\n",
        "            # Selecting all other layers except \"first_hidden_layer\"\n",
        "            if \"first_hidden_layer\" not in module.__str__():\n",
        "                w = module.weight.data\n",
        "                # Perform exponentiation operation on negative weights\n",
        "                w[w < 0] = torch.exp(w[w < 0]-self.epsilon)\n",
        "                module.weight.data = w # Storing the updated weights\n"
      ],
      "metadata": {
        "id": "YXjjQx8lpz0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "jDGPxLPfpz0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457078a6-d3ec-4c7e-863d-a4a9a27c78df",
        "id": "4AARJG-Vpz0s"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss : 2.188\n",
            "Training_Acc  : 0.242\n",
            "Validation_Acc  : 0.327\n",
            "update: 0.3268\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Loss : 2.025\n",
            "Training_Acc  : 0.448\n",
            "Validation_Acc  : 0.539\n",
            "update: 0.2126\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Loss : 1.877\n",
            "Training_Acc  : 0.609\n",
            "Validation_Acc  : 0.662\n",
            "update: 0.1226\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Loss : 1.790\n",
            "Training_Acc  : 0.696\n",
            "Validation_Acc  : 0.730\n",
            "update: 0.0684\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Loss : 1.740\n",
            "Training_Acc  : 0.745\n",
            "Validation_Acc  : 0.766\n",
            "update: 0.0359\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Loss : 1.708\n",
            "Training_Acc  : 0.777\n",
            "Validation_Acc  : 0.792\n",
            "update: 0.0256\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Loss : 1.682\n",
            "Training_Acc  : 0.802\n",
            "Validation_Acc  : 0.814\n",
            "update: 0.0221\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Loss : 1.662\n",
            "Training_Acc  : 0.822\n",
            "Validation_Acc  : 0.832\n",
            "update: 0.0183\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Loss : 1.648\n",
            "Training_Acc  : 0.835\n",
            "Validation_Acc  : 0.839\n",
            "update: 0.0066\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Loss : 1.635\n",
            "Training_Acc  : 0.846\n",
            "Validation_Acc  : 0.849\n",
            "update: 0.0101\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Loss : 1.625\n",
            "Training_Acc  : 0.855\n",
            "Validation_Acc  : 0.857\n",
            "update: 0.0078\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Loss : 1.616\n",
            "Training_Acc  : 0.863\n",
            "Validation_Acc  : 0.863\n",
            "update: 0.0065\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Loss : 1.609\n",
            "Training_Acc  : 0.870\n",
            "Validation_Acc  : 0.867\n",
            "update: 0.0033\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Loss : 1.604\n",
            "Training_Acc  : 0.874\n",
            "Validation_Acc  : 0.870\n",
            "update: 0.0036\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Loss : 1.599\n",
            "Training_Acc  : 0.878\n",
            "Validation_Acc  : 0.876\n",
            "update: 0.0057\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Loss : 1.596\n",
            "Training_Acc  : 0.880\n",
            "Validation_Acc  : 0.875\n",
            "update: -0.0007\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Loss : 1.592\n",
            "Training_Acc  : 0.884\n",
            "Validation_Acc  : 0.881\n",
            "update: 0.0061\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Loss : 1.588\n",
            "Training_Acc  : 0.886\n",
            "Validation_Acc  : 0.885\n",
            "update: 0.0036\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Loss : 1.585\n",
            "Training_Acc  : 0.889\n",
            "Validation_Acc  : 0.888\n",
            "update: 0.0028\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Loss : 1.582\n",
            "Training_Acc  : 0.891\n",
            "Validation_Acc  : 0.887\n",
            "update: -0.0009\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Loss : 1.581\n",
            "Training_Acc  : 0.893\n",
            "Validation_Acc  : 0.891\n",
            "update: 0.0042\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Loss : 1.578\n",
            "Training_Acc  : 0.895\n",
            "Validation_Acc  : 0.891\n",
            "update: 0.0004\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Loss : 1.576\n",
            "Training_Acc  : 0.896\n",
            "Validation_Acc  : 0.892\n",
            "update: 0.0003\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Loss : 1.574\n",
            "Training_Acc  : 0.898\n",
            "Validation_Acc  : 0.893\n",
            "update: 0.0016\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Loss : 1.573\n",
            "Training_Acc  : 0.900\n",
            "Validation_Acc  : 0.893\n",
            "update: 0.0002\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Loss : 1.572\n",
            "Training_Acc  : 0.899\n",
            "Validation_Acc  : 0.895\n",
            "update: 0.0011\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Loss : 1.570\n",
            "Training_Acc  : 0.902\n",
            "Validation_Acc  : 0.898\n",
            "update: 0.0034\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Loss : 1.569\n",
            "Training_Acc  : 0.903\n",
            "Validation_Acc  : 0.898\n",
            "update: 0.0004\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Loss : 1.567\n",
            "Training_Acc  : 0.904\n",
            "Validation_Acc  : 0.900\n",
            "update: 0.0017\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Loss : 1.566\n",
            "Training_Acc  : 0.905\n",
            "Validation_Acc  : 0.901\n",
            "update: 0.0007\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Loss : 1.566\n",
            "Training_Acc  : 0.906\n",
            "Validation_Acc  : 0.901\n",
            "update: 0.0003\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Loss : 1.565\n",
            "Training_Acc  : 0.906\n",
            "Validation_Acc  : 0.898\n",
            "update: -0.0029\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Loss : 1.565\n",
            "Training_Acc  : 0.906\n",
            "Validation_Acc  : 0.901\n",
            "update: 0.0025\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Loss : 1.562\n",
            "Training_Acc  : 0.908\n",
            "Validation_Acc  : 0.902\n",
            "update: 0.0014\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Loss : 1.561\n",
            "Training_Acc  : 0.909\n",
            "Validation_Acc  : 0.903\n",
            "update: 0.0009\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Loss : 1.561\n",
            "Training_Acc  : 0.910\n",
            "Validation_Acc  : 0.906\n",
            "update: 0.0030\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Loss : 1.561\n",
            "Training_Acc  : 0.910\n",
            "Validation_Acc  : 0.904\n",
            "update: -0.0023\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Loss : 1.560\n",
            "Training_Acc  : 0.910\n",
            "Validation_Acc  : 0.906\n",
            "update: 0.0027\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Loss : 1.558\n",
            "Training_Acc  : 0.912\n",
            "Validation_Acc  : 0.908\n",
            "update: 0.0016\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Loss : 1.559\n",
            "Training_Acc  : 0.912\n",
            "Validation_Acc  : 0.906\n",
            "update: -0.0015\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Loss : 1.558\n",
            "Training_Acc  : 0.912\n",
            "Validation_Acc  : 0.907\n",
            "update: 0.0006\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Loss : 1.558\n",
            "Training_Acc  : 0.913\n",
            "Validation_Acc  : 0.911\n",
            "update: 0.0044\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Loss : 1.556\n",
            "Training_Acc  : 0.914\n",
            "Validation_Acc  : 0.906\n",
            "update: -0.0052\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Loss : 1.556\n",
            "Training_Acc  : 0.914\n",
            "Validation_Acc  : 0.907\n",
            "update: 0.0012\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Loss : 1.556\n",
            "Training_Acc  : 0.914\n",
            "Validation_Acc  : 0.908\n",
            "update: 0.0007\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Loss : 1.555\n",
            "Training_Acc  : 0.915\n",
            "Validation_Acc  : 0.907\n",
            "update: -0.0012\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Loss : 1.555\n",
            "Training_Acc  : 0.915\n",
            "Validation_Acc  : 0.908\n",
            "update: 0.0007\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Loss : 1.554\n",
            "Training_Acc  : 0.915\n",
            "Validation_Acc  : 0.912\n",
            "update: 0.0038\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Loss : 1.554\n",
            "Training_Acc  : 0.915\n",
            "Validation_Acc  : 0.912\n",
            "update: 0.0002\n",
            "-------------------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Loss : 1.554\n",
            "Training_Acc  : 0.916\n",
            "Validation_Acc  : 0.907\n",
            "update: -0.0050\n",
            "-------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "ioc_nn = IOC_NN()\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "# Slowing down the learning rate decay using beta values\n",
        "optimizer = torch.optim.Adam(ioc_nn.parameters(), lr=1e-4,betas=(0.9,0.9))\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 50\n",
        "\n",
        "\n",
        "while(epoch <max_epochs and tol_epochs<2):\n",
        "    epoch+=1\n",
        "    print(f'Epoch {epoch}')\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    tolerance = 1e-4\n",
        "    val_acc = []\n",
        "    # Training\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "      inputs, targets = data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = ioc_nn(inputs)\n",
        "\n",
        "\n",
        "      loss = loss_fun(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      ioc_nn.apply(WeightExponentiation())\n",
        "\n",
        "      current_loss.append(loss.item())\n",
        "\n",
        "      pred = torch.max(outputs,1).indices\n",
        "      acc= (targets == pred).sum().item()\n",
        "      current_acc.append(acc/targets.shape[0])\n",
        "\n",
        "    # Validation\n",
        "    for i,data in enumerate(val_loader,0):\n",
        "      inputs,targets = data\n",
        "      val_outputs = ioc_nn(inputs)\n",
        "      pred = torch.max(val_outputs,1).indices\n",
        "      acc =(targets==pred).sum().item()\n",
        "      val_acc.append(acc/targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "\n",
        "    print('Loss : %.3f' %(sum(current_loss) /len(current_loss)))\n",
        "    print('Training_Acc  : %.3f'%(sum(current_acc)/len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f'%(sum(val_acc)/len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"-------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if(update>0 and update<tolerance):\n",
        "      tol_epochs+=1\n",
        "    else:\n",
        "      tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc)/len(val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "pc5sY5zdpz0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = ioc_nn(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d9bf6b-b267-4a3b-d30c-2aedc6ea8eaf",
        "id": "oIxqO2Zqpz0s"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_Acc  : 0.913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ivamPHEo_z0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}