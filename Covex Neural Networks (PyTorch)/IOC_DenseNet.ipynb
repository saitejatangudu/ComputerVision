{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from typing import Any, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from torch import Tensor\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "fOQEf1y4lLyL",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:36:58.568011Z",
          "iopub.execute_input": "2023-11-17T21:36:58.568813Z",
          "iopub.status.idle": "2023-11-17T21:36:58.575110Z",
          "shell.execute_reply.started": "2023-11-17T21:36:58.568773Z",
          "shell.execute_reply": "2023-11-17T21:36:58.574011Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrate experiments on baseline DenseNet and its Input Ouput Convex(IOC) counterpart. Here's a summary of the key components and experiments covered:\n",
        "\n",
        "Exp 1: **DenseNet and IOC-DenseNet FOR CIFAR-10**:\n",
        "   - Implementing baseline architecture of DenseNet and constructing an IOC-DenseNet architecture enforcing convexity constraints similar to that of IOC-NN.\n",
        "\n",
        "Exp 2: **Training on Duplicate Free data (ciFAIR10)**:\n",
        "   - We use the same implementation of DenseNet and IOC-DenseNet on ciFAIR10 data set.\n",
        "\n",
        "Exp 3: **BoostedEnsemble**:\n",
        "\n",
        "- **Initialization:** ExpertEnsemble setup with base model and expert count.\n",
        "- **Training:** Experts trained separately with bootstrapped data, updating weights based on performance.   "
      ],
      "metadata": {
        "id": "NVqBmgHZEHK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 1. DenseNet and IOC-DenseNet on CIFAR10"
      ],
      "metadata": {
        "id": "tWx2Y_zI_IkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNet"
      ],
      "metadata": {
        "id": "MhOiTcv6oqij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''This class has been taken from the original source code of the DenseNet'''\n",
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self, num_input_features: int, growth_rate: int, bn_size: int, drop_rate: float, memory_efficient: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))\n",
        "        return bottleneck_output\n",
        "\n",
        "    # todo: rewrite when torchscript supports any\n",
        "    def any_requires_grad(self, input: List[Tensor]) -> bool:\n",
        "        for tensor in input:\n",
        "            if tensor.requires_grad:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\n",
        "        def closure(*inputs):\n",
        "            return self.bn_function(inputs)\n",
        "\n",
        "        return cp.checkpoint(closure, *input)\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
        "            if torch.jit.is_scripting():\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
        "\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
        "        else:\n",
        "            bottleneck_output = self.bn_function(prev_features)\n",
        "\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return new_features"
      ],
      "metadata": {
        "id": "qFFginQYnWFa",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:36:58.577264Z",
          "iopub.execute_input": "2023-11-17T21:36:58.577603Z",
          "iopub.status.idle": "2023-11-17T21:36:58.594732Z",
          "shell.execute_reply.started": "2023-11-17T21:36:58.577575Z",
          "shell.execute_reply": "2023-11-17T21:36:58.593737Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''This class has been taken from the original source code of the DenseNet'''\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int,\n",
        "        num_input_features: int,\n",
        "        bn_size: int,\n",
        "        growth_rate: int,\n",
        "        drop_rate: float,\n",
        "        memory_efficient: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features: Tensor) -> Tensor:\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)"
      ],
      "metadata": {
        "id": "ht_WKK0LnjiC",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:36:58.596463Z",
          "iopub.execute_input": "2023-11-17T21:36:58.597093Z",
          "iopub.status.idle": "2023-11-17T21:36:58.608531Z",
          "shell.execute_reply.started": "2023-11-17T21:36:58.597065Z",
          "shell.execute_reply": "2023-11-17T21:36:58.607645Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''This class has been taken from the original source code of the DenseNet'''\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features: int, num_output_features: int) -> None:\n",
        "        super().__init__()\n",
        "        self.norm = nn.BatchNorm2d(num_input_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)"
      ],
      "metadata": {
        "id": "2Sj_k1Ulnno-",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:36:58.612930Z",
          "iopub.execute_input": "2023-11-17T21:36:58.613761Z",
          "iopub.status.idle": "2023-11-17T21:36:58.620696Z",
          "shell.execute_reply.started": "2023-11-17T21:36:58.613725Z",
          "shell.execute_reply": "2023-11-17T21:36:58.619784Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''This class has been taken from the original source code of the DenseNet'''\n",
        "class DenseNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        growth_rate: int = 24,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0,\n",
        "        num_classes: int = 1000,\n",
        "        memory_efficient: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "                    (\"norm0\", nn.BatchNorm2d(num_init_features)),\n",
        "                    (\"relu0\", nn.ReLU(inplace=True)),\n",
        "                    (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module(\"transition%d\" % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "oPC5pMwelCYm",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:36:58.622324Z",
          "iopub.execute_input": "2023-11-17T21:36:58.622699Z",
          "iopub.status.idle": "2023-11-17T21:36:58.637619Z",
          "shell.execute_reply.started": "2023-11-17T21:36:58.622667Z",
          "shell.execute_reply": "2023-11-17T21:36:58.636663Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Trian and Test data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "# Spliting train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for all the train,val and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "UulG-HIooNZK",
        "outputId": "8ceca70b-e4a3-42ef-9bdf-1d74786a374b",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:36:58.686355Z",
          "iopub.execute_input": "2023-11-17T21:36:58.686604Z",
          "iopub.status.idle": "2023-11-17T21:37:00.185457Z",
          "shell.execute_reply.started": "2023-11-17T21:36:58.686584Z",
          "shell.execute_reply": "2023-11-17T21:37:00.184427Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model\n",
        "mlp = DenseNet(growth_rate=12,\n",
        "               block_config=(6, 12, 24, 16),\n",
        "               num_init_features=64,\n",
        "               bn_size=4,\n",
        "               drop_rate=0,\n",
        "               num_classes=10)\n",
        "\n",
        "# Loss function\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "# Initialize variables for tracking epochs and tolerance\n",
        "epoch = 0\n",
        "tol_epochs = 0\n",
        "max_epochs = 2\n",
        "prev_val_acc = 0.0\n",
        "\n",
        "# Training loop\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    # Training\n",
        "    mlp.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp(inputs)\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss.append(loss.item())\n",
        "\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        acc = (pred == targets).float().mean().item()\n",
        "        current_acc.append(acc)\n",
        "\n",
        "    # Validation\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            val_outputs = mlp(inputs)\n",
        "            _, pred = torch.max(val_outputs, 1)\n",
        "            acc = (pred == targets).float().mean().item()\n",
        "            val_acc.append(acc)\n",
        "\n",
        "    # Print training and validation performance\n",
        "    print('Loss: %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "    print('Training Accuracy: %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "    print('Validation Accuracy: %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "    # Early stopping criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('Update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if 0 < update < 1e-4:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc) / len(val_acc)"
      ],
      "metadata": {
        "id": "EMY68PkWotnp",
        "outputId": "230dcfc2-a3d6-4e61-a781-70dc1c233361",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:37:00.187262Z",
          "iopub.execute_input": "2023-11-17T21:37:00.187547Z",
          "iopub.status.idle": "2023-11-17T21:40:46.168310Z",
          "shell.execute_reply.started": "2023-11-17T21:37:00.187523Z",
          "shell.execute_reply": "2023-11-17T21:40:46.167122Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 1.983\nTraining_Acc  : 0.283\nValidation_Acc  : 0.370\nupdate: 0.3696\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 1.627\nTraining_Acc  : 0.408\nValidation_Acc  : 0.422\nupdate: 0.0526\n--------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "id": "4st9YtiKr5gm",
        "outputId": "d75f3390-acc8-40fb-ced0-772781a76914",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:46.170301Z",
          "iopub.execute_input": "2023-11-17T21:40:46.170850Z",
          "iopub.status.idle": "2023-11-17T21:40:56.972470Z",
          "shell.execute_reply.started": "2023-11-17T21:40:46.170814Z",
          "shell.execute_reply": "2023-11-17T21:40:56.971460Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test_Acc  : 0.432\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC-DenseNet"
      ],
      "metadata": {
        "id": "8i-tkGjdto1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following modifications are incorporated to the above considered predefined classes\n",
        "\n",
        "1. **Customized Initial Convolutional Layer**:\n",
        "   - Increased the number of filters in the initial convolutional layer (`conv0`) from 3 to 128 in the `IOC_DenseNet` class, enhancing the model's capacity to capture complex patterns from input images.\n",
        "\n",
        "2. **Activation Function Modification**:\n",
        "   - Replaced ReLU activation functions with ELU (Exponential Linear Unit) activation functions after batch normalization in the initial convolutional layer of the `IOC_DenseNet` class. ELU may offer advantages over ReLU, such as reduced vanishing gradient problems and improved learning dynamics.\n",
        "\n",
        "3. **Adjustments in Transition Layers**:\n",
        "   - Modified the transition layers in the `IOC_DenseNet` class to maintain the increased number of filters in the first convolutional block. This ensures seamless transition between dense blocks while preserving feature maps' dimensions and depth.\n",
        "\n",
        "5. **Consistent Initialization**:\n",
        "   - Ensured consistent weight initialization across different layers of the network using techniques such as Kaiming initialization for convolutional layers and constant initialization for batch normalization and linear layers. This helps in stabilizing the training process and improving convergence."
      ],
      "metadata": {
        "id": "Oda9FY9tCN7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self, num_input_features: int, growth_rate: int, bn_size: int, drop_rate: float, memory_efficient: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Batch normalization and ELU activation before the first convolutional layer\n",
        "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
        "        self.elu1 = nn.ELU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "        # Batch normalization and ELU activation before the second convolutional layer\n",
        "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
        "        self.elu2 = nn.ELU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Dropout rate and memory_efficient flag\n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    # Function to perform batch normalization, ELU activation, and convolution\n",
        "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.elu1(self.norm1(concated_features)))\n",
        "        return bottleneck_output\n",
        "\n",
        "    # Method to check if any tensor in the input list requires gradients\n",
        "    # Currently unused, may be used for future optimizations\n",
        "    def any_requires_grad(self, input: List[Tensor]) -> bool:\n",
        "        for tensor in input:\n",
        "            if tensor.requires_grad:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # Method for memory-efficient training using checkpointing\n",
        "    @torch.jit.unused\n",
        "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\n",
        "        def closure(*inputs):\n",
        "            return self.bn_function(inputs)\n",
        "\n",
        "        return cp.checkpoint(closure, *input)\n",
        "\n",
        "    # Forward pass through the DenseLayer\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        # Check if memory-efficient training is enabled and any tensor requires gradients\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
        "            if torch.jit.is_scripting():\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
        "\n",
        "            # Use checkpointing for memory-efficient training\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
        "        else:\n",
        "            # Compute bottleneck output directly\n",
        "            bottleneck_output = self.bn_function(prev_features)\n",
        "\n",
        "        # Compute new features using the second convolutional layer\n",
        "        new_features = self.conv2(self.elu2(self.norm2(bottleneck_output)))\n",
        "\n",
        "        # Apply dropout if applicable\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return new_features"
      ],
      "metadata": {
        "id": "Unys3cTJts2g",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:56.974631Z",
          "iopub.execute_input": "2023-11-17T21:40:56.974980Z",
          "iopub.status.idle": "2023-11-17T21:40:56.988239Z",
          "shell.execute_reply.started": "2023-11-17T21:40:56.974949Z",
          "shell.execute_reply": "2023-11-17T21:40:56.987338Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int,\n",
        "        num_input_features: int,\n",
        "        bn_size: int,\n",
        "        growth_rate: int,\n",
        "        drop_rate: float,\n",
        "        memory_efficient: bool = False,\n",
        "\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "\n",
        "            )\n",
        "            self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features: Tensor) -> Tensor:\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)"
      ],
      "metadata": {
        "id": "40WOR66NuTBo",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:56.989304Z",
          "iopub.execute_input": "2023-11-17T21:40:56.989609Z",
          "iopub.status.idle": "2023-11-17T21:40:57.004384Z",
          "shell.execute_reply.started": "2023-11-17T21:40:56.989575Z",
          "shell.execute_reply": "2023-11-17T21:40:57.003578Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features: int, num_output_features: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Batch normalization layer\n",
        "        self.norm = nn.BatchNorm2d(num_input_features)\n",
        "\n",
        "        # ELU activation layer inplace\n",
        "        self.elu = nn.ELU(inplace=True)\n",
        "\n",
        "        # Convolutional layer with kernel size 1x1\n",
        "        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "        # Average pooling layer with kernel size 2x2 and stride 2\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n"
      ],
      "metadata": {
        "id": "hcan0PVsuuZ1",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:57.005376Z",
          "iopub.execute_input": "2023-11-17T21:40:57.005631Z",
          "iopub.status.idle": "2023-11-17T21:40:57.015982Z",
          "shell.execute_reply.started": "2023-11-17T21:40:57.005609Z",
          "shell.execute_reply": "2023-11-17T21:40:57.015205Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IOC_DenseNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        growth_rate: int = 24,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0,\n",
        "        num_classes: int = 10,\n",
        "        memory_efficient: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Customized first convolution with increased number of filters\n",
        "        self.conv = OrderedDict(\n",
        "            [\n",
        "                (\"conv0\", nn.Conv2d(3, 128, kernel_size=7, stride=2, padding=3, bias=False)),  # Increased No of Filters of the first conv layer\n",
        "                (\"norm0\", nn.BatchNorm2d(128)),\n",
        "                (\"elu0\", nn.ELU(inplace=True)),\n",
        "                (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(self.conv)\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            num_features = 128 if i == 0 else num_features  # Increasing no of filters in first layer\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module(\"transition%d\" % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "gwB0y66_vXPv",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:57.016981Z",
          "iopub.execute_input": "2023-11-17T21:40:57.017231Z",
          "iopub.status.idle": "2023-11-17T21:40:57.032220Z",
          "shell.execute_reply.started": "2023-11-17T21:40:57.017209Z",
          "shell.execute_reply": "2023-11-17T21:40:57.031421Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to perform whitening Transform\n",
        "class WhiteningTransform:\n",
        "    def __init__(self):\n",
        "        self.whitening = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        mean = data.mean(dim=0)\n",
        "        centered_data = data - mean\n",
        "\n",
        "        cov_matrix = torch.mm(centered_data.t(), centered_data) / data.size(0)\n",
        "        U, S, V = torch.svd(cov_matrix)\n",
        "        self.whitening = torch.mm(centered_data, U) / torch.sqrt(S + 1e-10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.whitening is not None:\n",
        "            x = self.whitening.t() @ x.reshape(x.size(0), -1).t()\n",
        "            x = x.t().reshape(x.size(0), 3, 32, 32)\n",
        "            return x\n",
        "        return x\n",
        "\n",
        "# Creating a transform object withwhitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    WhiteningTransform()\n",
        "])"
      ],
      "metadata": {
        "id": "5-DNBz9MwuyW",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:57.033292Z",
          "iopub.execute_input": "2023-11-17T21:40:57.033616Z",
          "iopub.status.idle": "2023-11-17T21:40:57.047152Z",
          "shell.execute_reply.started": "2023-11-17T21:40:57.033584Z",
          "shell.execute_reply": "2023-11-17T21:40:57.046344Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Trian and Test data\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "# Spliting train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for all the train,val and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "Nv-yO9WzwoLn",
        "outputId": "3358ad71-9789-4195-b82a-3c31ecbccf4e",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:57.048288Z",
          "iopub.execute_input": "2023-11-17T21:40:57.048558Z",
          "iopub.status.idle": "2023-11-17T21:40:58.568125Z",
          "shell.execute_reply.started": "2023-11-17T21:40:57.048536Z",
          "shell.execute_reply": "2023-11-17T21:40:58.567280Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Weight Exponentiation\n",
        "class WeightExponentiation(object):\n",
        "\n",
        "    def __init__(self, epsilon = 5):\n",
        "        self.epsilon = epsilon # epsilon for constraining exponentiation of weights\n",
        "\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if hasattr(module, 'weight'):\n",
        "            # Selecting all other layers except \"first_hidden_layer\"\n",
        "            if \"conv\" not in module.__str__():\n",
        "                w = module.weight.data\n",
        "                # Perform exponentiation operation on negative weights\n",
        "                w[w < 0] = torch.exp(w[w < 0]-self.epsilon)\n",
        "                module.weight.data = w # Storing the updated weights\n"
      ],
      "metadata": {
        "id": "FZxrk_8LxIZ-",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:58.572323Z",
          "iopub.execute_input": "2023-11-17T21:40:58.572761Z",
          "iopub.status.idle": "2023-11-17T21:40:58.579060Z",
          "shell.execute_reply.started": "2023-11-17T21:40:58.572736Z",
          "shell.execute_reply": "2023-11-17T21:40:58.578134Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = IOC_DenseNet(growth_rate = 12,\n",
        "        block_config = (6, 12, 24, 16),\n",
        "        num_init_features = 64,\n",
        "        bn_size = 4,\n",
        "        drop_rate = 0,\n",
        "        num_classes = 10,\n",
        "        )\n",
        "\n",
        "\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4,betas=(0.9,0.9))\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 2\n",
        "\n",
        "while(epoch <max_epochs and tol_epochs<2):\n",
        "    epoch+=1\n",
        "    print(f'Epoch {epoch}')\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    tolerance = 1e-4\n",
        "    val_acc = []\n",
        "\n",
        "    # Training\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        mlp.apply(WeightExponentiation())\n",
        "\n",
        "        current_loss.append(loss.item())\n",
        "\n",
        "        pred = torch.max(outputs,1).indices\n",
        "        acc= (targets == pred).sum().item()\n",
        "        current_acc.append(acc/targets.shape[0])\n",
        "\n",
        "    # Validation\n",
        "    for i,data in enumerate(val_loader,0):\n",
        "        inputs,targets = data\n",
        "        val_outputs = mlp(inputs)\n",
        "        pred = torch.max(val_outputs,1).indices\n",
        "        acc =(targets==pred).sum().item()\n",
        "        val_acc.append(acc/targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "\n",
        "    print('Loss : %.3f' %(sum(current_loss) /len(current_loss)))\n",
        "    print('Training_Acc  : %.3f'%(sum(current_acc)/len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f'%(sum(val_acc)/len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if(update>0 and update<tolerance):\n",
        "        tol_epochs+=1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc)/len(val_acc)"
      ],
      "metadata": {
        "id": "1zYvJWPiw3fE",
        "outputId": "67078c01-b203-4654-88d8-04246856010e",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:40:58.580202Z",
          "iopub.execute_input": "2023-11-17T21:40:58.580499Z",
          "iopub.status.idle": "2023-11-17T21:45:54.707077Z",
          "shell.execute_reply.started": "2023-11-17T21:40:58.580475Z",
          "shell.execute_reply": "2023-11-17T21:45:54.705228Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 2.285\nTraining_Acc  : 0.135\nValidation_Acc  : 0.142\nupdate: 0.1421\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.272\nTraining_Acc  : 0.144\nValidation_Acc  : 0.155\nupdate: 0.0133\n--------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "id": "lJ5FbFCMxhum",
        "execution": {
          "iopub.status.busy": "2023-11-17T21:45:54.709143Z",
          "iopub.execute_input": "2023-11-17T21:45:54.709507Z",
          "iopub.status.idle": "2023-11-17T21:46:09.492777Z",
          "shell.execute_reply.started": "2023-11-17T21:45:54.709462Z",
          "shell.execute_reply": "2023-11-17T21:46:09.491792Z"
        },
        "trusted": true,
        "outputId": "bf56f729-ac64-483a-91c7-8dc6d4051930"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test_Acc  : 0.155\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 2. Training on Duplicate Free data (ciFAIR10)"
      ],
      "metadata": {
        "id": "2Zy__BkTAJMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNet"
      ],
      "metadata": {
        "id": "4ycUWVNuSV_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets\n",
        "\n",
        "class ciFAIR10(torchvision.datasets.CIFAR10):\n",
        "    base_folder = 'ciFAIR-10'\n",
        "    url = 'https://github.com/cvjena/cifair/releases/download/v1.0/ciFAIR-10.zip'\n",
        "    filename = 'ciFAIR-10.zip'\n",
        "    tgz_md5 = 'ca08fd390f0839693d3fc45c4e49585f'\n",
        "    test_list = [\n",
        "        ['test_batch', '01290e6b622a1977a000eff13650aca2'],\n",
        "    ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T21:46:09.494157Z",
          "iopub.execute_input": "2023-11-17T21:46:09.494539Z",
          "iopub.status.idle": "2023-11-17T21:46:09.499367Z",
          "shell.execute_reply.started": "2023-11-17T21:46:09.494506Z",
          "shell.execute_reply": "2023-11-17T21:46:09.498483Z"
        },
        "trusted": true,
        "id": "JgLgxL5cSV_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Trian and Test data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = ciFAIR10('data', train=True, download=True, transform=transform)\n",
        "test_data = ciFAIR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "# Spliting train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for all the train,val and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T21:46:09.500513Z",
          "iopub.execute_input": "2023-11-17T21:46:09.501499Z",
          "iopub.status.idle": "2023-11-17T21:46:11.167379Z",
          "shell.execute_reply.started": "2023-11-17T21:46:09.501466Z",
          "shell.execute_reply": "2023-11-17T21:46:11.166583Z"
        },
        "trusted": true,
        "id": "hC6YRm6cSV_H",
        "outputId": "88f35999-23d2-4d01-cc5b-d58f9cfbc428"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Initialize DenseNet model\n",
        "mlp = DenseNet(growth_rate=12,\n",
        "               block_config=(6, 12, 24, 16),\n",
        "               num_init_features=64,\n",
        "               bn_size=4,\n",
        "               drop_rate=0,\n",
        "               num_classes=10)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "# Initialize variables for training loop\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 2\n",
        "\n",
        "# Main training loop\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "\n",
        "    # Lists to store loss and accuracy for training and validation\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    # Training loop\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp(inputs)\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss.append(loss.item())\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Validation loop\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = mlp(inputs)\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        val_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "    print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "    print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Check for improvement in validation accuracy\n",
        "    if update > 0 and update < tolerance:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc) / len(val_acc)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T21:46:11.168665Z",
          "iopub.execute_input": "2023-11-17T21:46:11.169637Z",
          "iopub.status.idle": "2023-11-17T21:50:10.729618Z",
          "shell.execute_reply.started": "2023-11-17T21:46:11.169599Z",
          "shell.execute_reply": "2023-11-17T21:50:10.728552Z"
        },
        "trusted": true,
        "id": "l4ZGvs2eSV_H",
        "outputId": "a501ecf7-f1ad-49e3-a4d4-645c6679c200"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 1.636\nTraining_Acc  : 0.425\nValidation_Acc  : 0.503\nupdate: 0.5025\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 1.267\nTraining_Acc  : 0.551\nValidation_Acc  : 0.560\nupdate: 0.0574\n--------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T21:50:10.731484Z",
          "iopub.execute_input": "2023-11-17T21:50:10.731858Z",
          "iopub.status.idle": "2023-11-17T21:50:22.597810Z",
          "shell.execute_reply.started": "2023-11-17T21:50:10.731823Z",
          "shell.execute_reply": "2023-11-17T21:50:22.596955Z"
        },
        "trusted": true,
        "id": "1vtPr723SV_H",
        "outputId": "d536decc-2371-4f7d-d533-b1cd493f03cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test_Acc  : 0.552\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC_DenseNet"
      ],
      "metadata": {
        "id": "pDFE84MjSV_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a transform object withwhitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    WhiteningTransform()\n",
        "])\n",
        "\n",
        "# Loading Trian and Test data\n",
        "train_data = ciFAIR10('data', train=True, download=True, transform=transform)\n",
        "test_data = ciFAIR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "# Spliting train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for all the train,val and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T21:50:22.599172Z",
          "iopub.execute_input": "2023-11-17T21:50:22.599761Z",
          "iopub.status.idle": "2023-11-17T21:50:24.263478Z",
          "shell.execute_reply.started": "2023-11-17T21:50:22.599724Z",
          "shell.execute_reply": "2023-11-17T21:50:24.262669Z"
        },
        "trusted": true,
        "id": "n2AoKYaiSV_H",
        "outputId": "048c25ff-ae9e-4e22-94da-354a8482b6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Initialize IOC_DenseNet model\n",
        "mlp = IOC_DenseNet(growth_rate=12,\n",
        "                   block_config=(6, 12, 24, 16),\n",
        "                   num_init_features=64,\n",
        "                   bn_size=4,\n",
        "                   drop_rate=0,\n",
        "                   num_classes=10)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, betas=(0.9, 0.9))\n",
        "\n",
        "# Initialize variables for training loop\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 2\n",
        "\n",
        "# Main training loop\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "\n",
        "    # Lists to store loss and accuracy for training and validation\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    # Training loop\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp(inputs)\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Apply weight exponentiation after each update\n",
        "        mlp.apply(WeightExponentiation())\n",
        "\n",
        "        current_loss.append(loss.item())\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Validation loop\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = mlp(inputs)\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        val_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "    print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "    print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Check for improvement in validation accuracy\n",
        "    if update > 0 and update < tolerance:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc) / len(val_acc)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T21:50:24.264793Z",
          "iopub.execute_input": "2023-11-17T21:50:24.265083Z",
          "iopub.status.idle": "2023-11-17T21:55:25.383813Z",
          "shell.execute_reply.started": "2023-11-17T21:50:24.265059Z",
          "shell.execute_reply": "2023-11-17T21:55:25.382683Z"
        },
        "trusted": true,
        "id": "xoI8GsRhSV_H",
        "outputId": "574757dd-2d9c-4027-fb2d-e80c1a108439"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 2.283\nTraining_Acc  : 0.139\nValidation_Acc  : 0.144\nupdate: 0.1438\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.270\nTraining_Acc  : 0.149\nValidation_Acc  : 0.147\nupdate: 0.0036\n--------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T21:55:25.385594Z",
          "iopub.execute_input": "2023-11-17T21:55:25.385984Z",
          "iopub.status.idle": "2023-11-17T21:55:40.117202Z",
          "shell.execute_reply.started": "2023-11-17T21:55:25.385958Z",
          "shell.execute_reply": "2023-11-17T21:55:40.116060Z"
        },
        "trusted": true,
        "id": "3p-MuNKlSV_I",
        "outputId": "799ad9a0-fe49-4ef4-f429-1480c69e6134"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test_Acc  : 0.153\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 3. Boosted Ensemble"
      ],
      "metadata": {
        "id": "_EDEHp3vE4On"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC-DenseNet"
      ],
      "metadata": {
        "id": "h2Ho5UtXDzmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:37.355763Z",
          "iopub.execute_input": "2023-11-20T13:41:37.356169Z",
          "iopub.status.idle": "2023-11-20T13:41:37.430713Z",
          "shell.execute_reply.started": "2023-11-20T13:41:37.356143Z",
          "shell.execute_reply": "2023-11-20T13:41:37.429756Z"
        },
        "trusted": true,
        "id": "CQnCML6wKZGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a transform object with whitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "   WhiteningTransform()\n",
        "])\n",
        "# Loading the MNIST dataset with whitening transformation\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "_sCY8Yr-5X25",
        "outputId": "378a2da0-ce7c-450a-8d1e-6a0b5cd32816",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:37.454863Z",
          "iopub.execute_input": "2023-11-20T13:41:37.455215Z",
          "iopub.status.idle": "2023-11-20T13:41:44.921383Z",
          "shell.execute_reply.started": "2023-11-20T13:41:37.455167Z",
          "shell.execute_reply": "2023-11-20T13:41:44.920401Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|| 170498071/170498071 [00:03<00:00, 43428993.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Weight Exponentiation\n",
        "class WeightExponentiation(object):\n",
        "\n",
        "    def __init__(self, epsilon = 5):\n",
        "        self.epsilon = epsilon # epsilon for constraining exponentiation of weights\n",
        "\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if hasattr(module, 'weight'):\n",
        "            # Selecting all other layers except \"first_hidden_layer\"\n",
        "            if \"first_hidden_layer\" not in module.__str__() and \"fc0\" not in module.__str__():\n",
        "                w = module.weight.data\n",
        "                # Perform exponentiation operation on negative weights\n",
        "                w[w < 0] = torch.exp(w[w < 0]-self.epsilon)\n",
        "                module.weight.data = w # Storing the updated weights\n"
      ],
      "metadata": {
        "id": "05dDBG1X6Ee4",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.922544Z",
          "iopub.execute_input": "2023-11-20T13:41:44.922832Z",
          "iopub.status.idle": "2023-11-20T13:41:44.929437Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.922807Z",
          "shell.execute_reply": "2023-11-20T13:41:44.928612Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Reshape(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom PyTorch module to reshape input tensors to a specified shape.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape):\n",
        "        \"\"\"\n",
        "        Constructor method to initialize the Reshape module.\n",
        "\n",
        "        Args:\n",
        "            shape (tuple): Desired shape for the output tensor.\n",
        "        \"\"\"\n",
        "        super(Reshape, self).__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Reshape module.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor to be reshaped.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Reshaped tensor with the specified shape.\n",
        "        \"\"\"\n",
        "        # Reshape the input tensor to the specified shape\n",
        "        return torch.reshape(x, self.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.931360Z",
          "iopub.execute_input": "2023-11-20T13:41:44.931668Z",
          "iopub.status.idle": "2023-11-20T13:41:44.952129Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.931635Z",
          "shell.execute_reply": "2023-11-20T13:41:44.951271Z"
        },
        "trusted": true,
        "id": "SbiCv094KZGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class ExpertEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    A class representing an ensemble of expert models for training.\n",
        "\n",
        "    Args:\n",
        "        base_learner (nn.Module): Base learner model used for individual experts.\n",
        "        num_experts (int): Number of experts in the ensemble.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_learner, num_experts: int = 3):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.experts = nn.ModuleList([base_learner().to(device) for _ in range(self.num_experts)])\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the ensemble model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            list: List of outputs from individual experts.\n",
        "        \"\"\"\n",
        "        expert_outputs = [expert(x) for expert in self.experts]\n",
        "        return expert_outputs\n",
        "\n",
        "    def train_experts(self, train_loader):\n",
        "        \"\"\"\n",
        "        Train individual experts in the ensemble.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "        \"\"\"\n",
        "        weights = torch.ones(len(train_loader.dataset))\n",
        "        for i in range(self.num_experts):\n",
        "            print(\"Expert:\", i)\n",
        "            self.train_single_expert(self.experts[i], train_loader)\n",
        "            weights = self.update_sample_weights(self.experts[i], train_loader.dataset, weights)\n",
        "            train_loader = self.bootstrap_dataloader(train_loader, weights)\n",
        "\n",
        "    def train_single_expert(self, model, train_loader):\n",
        "        \"\"\"\n",
        "        Train a single expert model.\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): Expert model to train.\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "        \"\"\"\n",
        "        model.train()\n",
        "        train_loader = DataLoader(train_loader.dataset, batch_size=64, shuffle=True)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=[0.9, 0.9])\n",
        "        max_epochs = 2\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            print(f'Epoch {epoch + 1}')\n",
        "            current_loss = []\n",
        "            current_acc = []\n",
        "\n",
        "            # Training\n",
        "            for i, data in enumerate(train_loader, 0):\n",
        "                inputs, targets = data\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                model.apply(WeightExponentiation())\n",
        "                current_loss.append(loss.item())\n",
        "\n",
        "                pred = torch.max(outputs, 1).indices\n",
        "                acc = (targets == pred).sum().item()\n",
        "                current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "            # Performance Evaluation\n",
        "            print('Loss: %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "            print('Training Accuracy: %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "\n",
        "    def update_sample_weights(self, weak_learner, dataset, sample_weights, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        Update sample weights based on weak learner predictions.\n",
        "\n",
        "        Args:\n",
        "            weak_learner (nn.Module): Weak learner model.\n",
        "            dataset (torch.utils.data.Dataset): Dataset used for training.\n",
        "            sample_weights (torch.Tensor): Current sample weights.\n",
        "            learning_rate (float): Learning rate for updating weights.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Updated sample weights.\n",
        "        \"\"\"\n",
        "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n",
        "\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            weak_learner.to(device)\n",
        "            outputs = weak_learner(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            misclassifications = (outputs.argmax(dim=1) != labels).float()\n",
        "            sample_weights = sample_weights.to(device)\n",
        "            sample_weights *= torch.exp(learning_rate * misclassifications)\n",
        "            sample_weights /= sample_weights.sum()\n",
        "\n",
        "        return sample_weights\n",
        "\n",
        "    def bootstrap_dataloader(self, dataloader, sample_weights):\n",
        "        \"\"\"\n",
        "        Bootstrap a DataLoader using weighted sampling.\n",
        "\n",
        "        Args:\n",
        "            dataloader (DataLoader): Original DataLoader.\n",
        "            sample_weights (torch.Tensor): Sample weights.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: Bootstrapped DataLoader.\n",
        "        \"\"\"\n",
        "        num_samples = len(dataloader.dataset)\n",
        "        bootstrap_indices = torch.multinomial(sample_weights, num_samples, replacement=True).int()\n",
        "        return DataLoader([dataloader.dataset[i] for i in bootstrap_indices])\n"
      ],
      "metadata": {
        "id": "cRK9EDGwOeQw",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.953986Z",
          "iopub.execute_input": "2023-11-20T13:41:44.954334Z",
          "iopub.status.idle": "2023-11-20T13:41:44.974792Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.954301Z",
          "shell.execute_reply": "2023-11-20T13:41:44.973736Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Boosting_Ensemble:\n",
        "    \"\"\"\n",
        "    A class for training a boosting ensemble with a gating network.\n",
        "\n",
        "    Args:\n",
        "        num_experts (int): Number of expert models in the ensemble.\n",
        "        base_learner (nn.Module): Base learner model used for individual experts.\n",
        "        gating_network (nn.Module): Gating network model.\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_experts, base_learner, gating_network, train_loader):\n",
        "        \"\"\"\n",
        "        Constructor method to initialize the Boosting_Ensemble.\n",
        "\n",
        "        Args:\n",
        "            num_experts (int): Number of expert models in the ensemble.\n",
        "            base_learner (nn.Module): Base learner model used for individual experts.\n",
        "            gating_network (nn.Module): Gating network model.\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "        \"\"\"\n",
        "        self.num_experts = num_experts\n",
        "        self.base_learner = base_learner\n",
        "        self.gating_network = gating_network(num_experts).to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.expert_training()\n",
        "        self.gn_training()\n",
        "        self.testing()\n",
        "\n",
        "    def expert_training(self):\n",
        "        \"\"\"\n",
        "        Train individual expert models in the ensemble.\n",
        "        \"\"\"\n",
        "        model = ExpertEnsemble(self.base_learner, self.num_experts)\n",
        "        model.train_experts(self.train_loader)\n",
        "        print(\"Expert Training Completed\")\n",
        "        self.trained_experts = model.experts\n",
        "\n",
        "        # Set requires_grad to True for all parameters of trained experts\n",
        "        for expert in self.trained_experts:\n",
        "            for param in expert.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def gn_training(self):\n",
        "        \"\"\"\n",
        "        Train the gating network.\n",
        "        \"\"\"\n",
        "        total_loss = 0.0\n",
        "        optimizer = torch.optim.Adam(self.gating_network.parameters(), lr=1e-4, betas=[0.9, 0.9])\n",
        "        epoch = 0\n",
        "        max_epochs = 5\n",
        "        loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "        while epoch < max_epochs:\n",
        "            epoch += 1\n",
        "            print(f'Epoch {epoch}')\n",
        "            current_loss = []\n",
        "            current_acc = []\n",
        "            tolerance = 1e-4\n",
        "\n",
        "            # Training\n",
        "            for i, data in enumerate(train_loader, 0):\n",
        "                inputs, targets = data\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                combined_outputs = torch.cat([i(inputs) for i in self.trained_experts], dim=1)\n",
        "\n",
        "                outputs = self.gating_network(combined_outputs)\n",
        "                loss = loss_fun(outputs, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                self.gating_network.apply(WeightExponentiation())\n",
        "                current_loss.append(loss.item())\n",
        "\n",
        "                pred = torch.max(outputs, 1).indices\n",
        "                acc = (targets == pred).sum().item()\n",
        "                current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "            # Performance Evaluation\n",
        "            print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "            print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "\n",
        "        print(\"Training of Gating Network Completed\")\n",
        "\n",
        "    def testing(self):\n",
        "        \"\"\"\n",
        "        Test the ensemble on a test dataset.\n",
        "        \"\"\"\n",
        "        test_acc = []\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            inputs, targets = data\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            combined_outputs = torch.cat([i(inputs) for i in self.trained_experts], dim=1)\n",
        "\n",
        "            test_outputs = self.gating_network(combined_outputs)\n",
        "\n",
        "            pred = torch.max(test_outputs, 1).indices\n",
        "            acc = (targets == pred).sum().item()\n",
        "            test_acc.append(acc / targets.shape[0])\n",
        "        print('Test_Acc  : %.3f' % (sum(test_acc) / len(test_acc)))"
      ],
      "metadata": {
        "id": "Bd1pC0D55N3o",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.976007Z",
          "iopub.execute_input": "2023-11-20T13:41:44.976280Z",
          "iopub.status.idle": "2023-11-20T13:41:45.000598Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.976256Z",
          "shell.execute_reply": "2023-11-20T13:41:44.999605Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from typing import Tuple\n",
        "\n",
        "class Dense_Gating(nn.Module):\n",
        "    \"\"\"\n",
        "    Gating network based on DenseNet architecture for IOC_Densenet ensemble.\n",
        "\n",
        "    Args:\n",
        "        growth_rate (int): Growth rate for DenseNet.\n",
        "        block_config (Tuple[int, int, int, int]): Configuration of dense blocks.\n",
        "        num_init_features (int): Number of initial features.\n",
        "        bn_size (int): Batch normalization size.\n",
        "        drop_rate (float): Dropout rate.\n",
        "        num_classes (int): Number of output classes.\n",
        "        memory_efficient (bool): Flag for memory-efficient computation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        growth_rate: int = 12,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0,\n",
        "        num_classes: int = 10,\n",
        "        memory_efficient: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the convolutional layers\n",
        "        self.conv = OrderedDict(\n",
        "            [\n",
        "                (\"fc0\", nn.Linear(3 * 10, 3 * 32 * 32)),  # Fully connected layer\n",
        "                (\"reshape\", Reshape((-1, 3, 32, 32))),  # Reshape to (batch_size, 3, 32, 32)\n",
        "                (\"conv0\", nn.Conv2d(3, 128, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "                (\"norm0\", nn.BatchNorm2d(128)),\n",
        "                (\"elu0\", nn.ELU(inplace=True)),\n",
        "                (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(self.conv)\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            num_features = 128 if i == 0 else num_features  # Increasing number of filters in first layer\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module(\"transition%d\" % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Initialize parameters\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the gating network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "EEhXfh7JI5Us",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:51.807797Z",
          "iopub.execute_input": "2023-11-20T13:41:51.808129Z",
          "iopub.status.idle": "2023-11-20T13:41:51.824624Z",
          "shell.execute_reply.started": "2023-11-20T13:41:51.808103Z",
          "shell.execute_reply": "2023-11-20T13:41:51.823653Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Boosting_Ensemble(3,IOC_DenseNet,Dense_Gating,train_loader)"
      ],
      "metadata": {
        "id": "Ci3pC58T_I34",
        "outputId": "984b27ab-39c3-43c8-b88b-810267f38868",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:59:34.344631Z",
          "iopub.execute_input": "2023-11-20T13:59:34.345289Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "STAGE:2023-11-20 13:59:34 47:47 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Expert: 0\nEpoch 1\nLoss: 2.268\nTraining Accuracy: 0.150\nEpoch 2\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}