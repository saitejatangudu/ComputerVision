{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrate experiments on baseline AllConv and its Input Ouput Convex(IOC) counterpart. Here's a summary of the key components and experiments covered:\n",
        "\n",
        "Exp 1: **AllConv and IOC-AllConv FOR CIFAR-10**:\n",
        "   - Implementing baseline architecture of AllConv and constructing an IOC-AllConv architecture enforcing convexity constraints similar to that of IOC-NN.\n",
        "\n",
        "Exp 2: **Training on Duplicate Free data (ciFAIR10)**:\n",
        "   - We use the same implementation of AllConv and IOC-AllConv on ciFAIR10 data set.\n",
        "\n",
        "Exp 3: **Partially randomized labeling**:\n",
        "   - We modify the original CIFAR10 dataset by randomly introducing noisy labels based on a specified noise percentage and report the performance of the models on the noise induced data sets.\n",
        "\n",
        "Exp 4: **BoostedEnsemble**:\n",
        "\n",
        "- **Initialization:** ExpertEnsemble setup with base model and expert count.\n",
        "- **Training:** Experts trained separately with bootstrapped data, updating weights based on performance.   "
      ],
      "metadata": {
        "id": "De5DD-HtMHd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "iOI-L9mOF2Db",
        "execution": {
          "iopub.status.busy": "2023-11-17T13:59:26.272297Z",
          "iopub.execute_input": "2023-11-17T13:59:26.273417Z",
          "iopub.status.idle": "2023-11-17T13:59:26.278109Z",
          "shell.execute_reply.started": "2023-11-17T13:59:26.273376Z",
          "shell.execute_reply": "2023-11-17T13:59:26.277201Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp1. AllConv and IOC-AllConv FOR CIFAR-10"
      ],
      "metadata": {
        "id": "CqDm6e1Qy8dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AllConv"
      ],
      "metadata": {
        "id": "oTo48t2hE1KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the AllCNN model class\n",
        "class AllCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define convolutional layers\n",
        "        self.layers = nn.Sequential(\n",
        "            # Input channels: 3, Output channels: 96, Kernel size: 3\n",
        "            nn.Conv2d(3, 96, 3),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU(),\n",
        "            # Input channels: 96, Output channels: 96, Kernel size: 3\n",
        "            nn.Conv2d(96, 96, 3),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU(),\n",
        "            # Input channels: 96, Output channels: 96, Kernel size: 3, Stride: 2 (downsampling)\n",
        "            nn.Conv2d(96, 96, 3, stride=2),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU(),\n",
        "            # Input channels: 96, Output channels: 192, Kernel size: 3\n",
        "            nn.Conv2d(96, 192, 3),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU(),\n",
        "            # Input channels: 192, Output channels: 192, Kernel size: 3\n",
        "            nn.Conv2d(192, 192, 3),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU(),\n",
        "            # Input channels: 192, Output channels: 192, Kernel size: 3, Stride: 2 (downsampling)\n",
        "            nn.Conv2d(192, 192, 3, stride=2),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU(),\n",
        "            # Input channels: 192, Output channels: 192, Kernel size: 3\n",
        "            nn.Conv2d(192, 192, 3),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU(),\n",
        "            # Input channels: 192, Output channels: 192, Kernel size: 1\n",
        "            nn.Conv2d(192, 192, 1),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU(),\n",
        "            # Input channels: 192, Output channels: 10, Kernel size: 1\n",
        "            nn.Conv2d(192, 10, 1),\n",
        "            # ReLU activation function\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Adaptive average pooling layer\n",
        "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Softmax activation function\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    # Forward method to define the forward pass of the model\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers\n",
        "        x = self.layers(x)\n",
        "\n",
        "        # Apply adaptive average pooling\n",
        "        x = self.pooling(x)\n",
        "\n",
        "        # Squeeze the tensor to remove dimensions of size 1\n",
        "        x = x.squeeze()\n",
        "\n",
        "        # Apply softmax activation function\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "bZ4XLjxntRiL",
        "execution": {
          "iopub.status.busy": "2023-11-17T13:59:26.280218Z",
          "iopub.execute_input": "2023-11-17T13:59:26.280738Z",
          "iopub.status.idle": "2023-11-17T13:59:26.292241Z",
          "shell.execute_reply.started": "2023-11-17T13:59:26.280695Z",
          "shell.execute_reply": "2023-11-17T13:59:26.291254Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "cfbKWKtYZUL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Trian and Test data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "# Spliting train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for all the train,val and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "PyUDa8g_ohaQ",
        "outputId": "8e1f280c-2749-4355-ff2a-18ffbc50e80c",
        "execution": {
          "iopub.status.busy": "2023-11-17T13:59:26.293587Z",
          "iopub.execute_input": "2023-11-17T13:59:26.293950Z",
          "iopub.status.idle": "2023-11-17T13:59:27.933049Z",
          "shell.execute_reply.started": "2023-11-17T13:59:26.293915Z",
          "shell.execute_reply": "2023-11-17T13:59:27.932151Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "TPmXd-SpzDAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "mlp = AllCNN()\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "# Initialize variables for tracking epochs and early stopping\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 2\n",
        "\n",
        "# Training loop\n",
        "while(epoch < max_epochs and tol_epochs < 2):\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    # Training\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        # Calculate loss and perform backpropagation\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss.append(loss.item())\n",
        "\n",
        "        # Calculate accuracy\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Validation\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = mlp(inputs)\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        val_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "    print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "    print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if update > 0 and update < tolerance:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc) / len(val_acc)"
      ],
      "metadata": {
        "id": "AhAVrW-RcRLx",
        "outputId": "45878718-d759-4112-bfb1-105f73d7bbc4",
        "execution": {
          "iopub.status.busy": "2023-11-17T13:59:27.934895Z",
          "iopub.execute_input": "2023-11-17T13:59:27.935334Z",
          "iopub.status.idle": "2023-11-17T14:09:46.180078Z",
          "shell.execute_reply.started": "2023-11-17T13:59:27.935291Z",
          "shell.execute_reply": "2023-11-17T14:09:46.178993Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 2.269\nTraining_Acc  : 0.170\nValidation_Acc  : 0.216\nupdate: 0.2158\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.212\nTraining_Acc  : 0.237\nValidation_Acc  : 0.258\nupdate: 0.0418\n--------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "LKNzxGclZiba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "id": "wmyaV2tIl9gB",
        "outputId": "883202c9-4076-46c4-bfed-b3371381b7e2",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:09:46.182962Z",
          "iopub.execute_input": "2023-11-17T14:09:46.183342Z",
          "iopub.status.idle": "2023-11-17T14:10:11.438592Z",
          "shell.execute_reply.started": "2023-11-17T14:09:46.183307Z",
          "shell.execute_reply": "2023-11-17T14:10:11.437450Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test_Acc  : 0.257\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC - AllConv"
      ],
      "metadata": {
        "id": "H66GupBRtbQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "UNXT0-eLZ1dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to perform whitening Transform\n",
        "class WhiteningTransform:\n",
        "    def __init__(self):\n",
        "        self.whitening = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        mean = data.mean(dim=0)\n",
        "        centered_data = data - mean\n",
        "\n",
        "        cov_matrix = torch.mm(centered_data.t(), centered_data) / data.size(0)\n",
        "        U, S, V = torch.svd(cov_matrix)\n",
        "        self.whitening = torch.mm(centered_data, U) / torch.sqrt(S + 1e-10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.whitening is not None:\n",
        "            x = self.whitening.t() @ x.reshape(x.size(0), -1).t()\n",
        "            x = x.t().reshape(x.size(0), 3, 32, 32)\n",
        "            return x\n",
        "        return x\n",
        "\n",
        "# Creating a transform object withwhitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    WhiteningTransform()\n",
        "])\n"
      ],
      "metadata": {
        "id": "frQiNyWx3OJn",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:10:11.440225Z",
          "iopub.execute_input": "2023-11-17T14:10:11.440923Z",
          "iopub.status.idle": "2023-11-17T14:10:11.451115Z",
          "shell.execute_reply.started": "2023-11-17T14:10:11.440878Z",
          "shell.execute_reply": "2023-11-17T14:10:11.449854Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loading the MNIST dataset with whitening transformation\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "H5LsppEAMwtP",
        "outputId": "a8671964-91de-43cf-b571-193989be38f6",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:10:11.452859Z",
          "iopub.execute_input": "2023-11-17T14:10:11.453234Z",
          "iopub.status.idle": "2023-11-17T14:10:13.203423Z",
          "shell.execute_reply.started": "2023-11-17T14:10:11.453201Z",
          "shell.execute_reply": "2023-11-17T14:10:13.202663Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "dJXgz0RSZ4H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class IOC_AllCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the first convolutional layer with extra filters\n",
        "        self.first_conv_layer = nn.Conv2d(3, 192, 3)\n",
        "\n",
        "        # Define the rest of the layers in the network\n",
        "        self.layers = nn.Sequential(\n",
        "            self.first_conv_layer,\n",
        "            nn.BatchNorm2d(192),  # Batch normalization layer\n",
        "            nn.ELU(True),         # ELU activation function\n",
        "\n",
        "            nn.Conv2d(192, 96, 3),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(96, 96, 3, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(96, 192, 3),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(192, 192, 3),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(192, 192, 3, stride=2),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(192, 192, 3),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(192, 192, 1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(192, 10, 1),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ELU(True)\n",
        "        )\n",
        "        # Adaptive average pooling layer\n",
        "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # Softmax activation function\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers\n",
        "        x = self.layers(x)\n",
        "        # Apply adaptive average pooling\n",
        "        x = self.pooling(x)\n",
        "        # Squeeze the tensor to remove dimensions of size 1\n",
        "        x = x.squeeze()\n",
        "        # Apply softmax activation function\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "gen7qqFAn_I8",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:10:13.204669Z",
          "iopub.execute_input": "2023-11-17T14:10:13.204971Z",
          "iopub.status.idle": "2023-11-17T14:10:13.215667Z",
          "shell.execute_reply.started": "2023-11-17T14:10:13.204946Z",
          "shell.execute_reply": "2023-11-17T14:10:13.214856Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Class for Weight Exponentiation\n",
        "class WeightExponentiation(object):\n",
        "    def __init__(self, epsilon=5):\n",
        "        \"\"\"\n",
        "        Initialize WeightExponentiation object.\n",
        "\n",
        "        Parameters:\n",
        "            epsilon (float): Epsilon for constraining exponentiation of weights.\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def __call__(self, module):\n",
        "        \"\"\"\n",
        "        Callable method to perform weight exponentiation operation on negative weights.\n",
        "\n",
        "        Parameters:\n",
        "            module (torch.nn.Module): Module to apply weight exponentiation operation.\n",
        "\n",
        "        \"\"\"\n",
        "        if hasattr(module, 'weight'):\n",
        "            # Selecting all layers except \"first_conv_layer\"\n",
        "            if \"first_conv_layer\" not in module.__str__():\n",
        "                w = module.weight.data\n",
        "                # Perform exponentiation operation on negative weights\n",
        "                w[w < 0] = torch.exp(w[w < 0] - self.epsilon)\n",
        "                module.weight.data = w  # Update the weights"
      ],
      "metadata": {
        "id": "q6y3mKNO3r0a",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:10:13.216839Z",
          "iopub.execute_input": "2023-11-17T14:10:13.217569Z",
          "iopub.status.idle": "2023-11-17T14:10:13.238833Z",
          "shell.execute_reply.started": "2023-11-17T14:10:13.217532Z",
          "shell.execute_reply": "2023-11-17T14:10:13.237842Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "JbxZsR3haUKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize the IOC_AllCNN model, loss function, optimizer, and other variables\n",
        "mlp = IOC_AllCNN()\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 2\n",
        "\n",
        "# Main training loop\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    tolerance = 1e-4\n",
        "    val_acc = []\n",
        "\n",
        "    # Training loop\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        # Calculate loss and perform backpropagation\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Apply weight exponentiation after each training iteration\n",
        "        mlp.apply(WeightExponentiation())\n",
        "        current_loss.append(loss.item())\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Validation loop\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = mlp(inputs)\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        val_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "    print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "    print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if update > 0 and update < tolerance:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc) / len(val_acc)"
      ],
      "metadata": {
        "id": "m64n9YditWVn",
        "outputId": "fc9734e5-cb55-4691-9900-adea9a7f68e4",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:10:13.240234Z",
          "iopub.execute_input": "2023-11-17T14:10:13.240574Z",
          "iopub.status.idle": "2023-11-17T14:29:03.300948Z",
          "shell.execute_reply.started": "2023-11-17T14:10:13.240546Z",
          "shell.execute_reply": "2023-11-17T14:29:03.299934Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 2.132\nTraining_Acc  : 0.444\nValidation_Acc  : 0.533\nupdate: 0.5330\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.047\nTraining_Acc  : 0.568\nValidation_Acc  : 0.584\nupdate: 0.0506\n--------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "fOnpBMcCaYw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "id": "9-4VPcbpj1WS",
        "outputId": "ee2f3eb1-4bde-42d3-f7ac-76f2a7c64eed",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:29:03.302585Z",
          "iopub.execute_input": "2023-11-17T14:29:03.303018Z",
          "iopub.status.idle": "2023-11-17T14:29:52.546557Z",
          "shell.execute_reply.started": "2023-11-17T14:29:03.302977Z",
          "shell.execute_reply": "2023-11-17T14:29:52.545451Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test_Acc  : 0.583\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp2. Training on Duplicate Free data (ciFAIR10)"
      ],
      "metadata": {
        "id": "qrdvvO0MEKuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AllConv"
      ],
      "metadata": {
        "id": "__kZAA7LHHhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets\n",
        "\n",
        "class ciFAIR10(torchvision.datasets.CIFAR10):\n",
        "    base_folder = 'ciFAIR-10'\n",
        "    url = 'https://github.com/cvjena/cifair/releases/download/v1.0/ciFAIR-10.zip'\n",
        "    filename = 'ciFAIR-10.zip'\n",
        "    tgz_md5 = 'ca08fd390f0839693d3fc45c4e49585f'\n",
        "    test_list = [\n",
        "        ['test_batch', '01290e6b622a1977a000eff13650aca2'],\n",
        "    ]"
      ],
      "metadata": {
        "id": "_q8xsePGEwBE",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:29:52.547998Z",
          "iopub.execute_input": "2023-11-17T14:29:52.548445Z",
          "iopub.status.idle": "2023-11-17T14:29:52.554585Z",
          "shell.execute_reply.started": "2023-11-17T14:29:52.548390Z",
          "shell.execute_reply": "2023-11-17T14:29:52.553592Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "vSHRqysGEf2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Trian and Test data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = ciFAIR10('data', train=True, download=True, transform=transform)\n",
        "test_data = ciFAIR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "# Spliting train data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for all the train,val and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "jUI76_yqEf2V",
        "outputId": "5ab82c82-e687-4721-a215-5f3a0fb1c05c",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:29:52.556194Z",
          "iopub.execute_input": "2023-11-17T14:29:52.556576Z",
          "iopub.status.idle": "2023-11-17T14:29:57.652646Z",
          "shell.execute_reply.started": "2023-11-17T14:29:52.556539Z",
          "shell.execute_reply": "2023-11-17T14:29:57.651486Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/168584397/527c7d80-2645-11e9-8008-a9ca4d2226ec?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231117T142952Z&X-Amz-Expires=300&X-Amz-Signature=6b5f6d46f27db1df1abb6217c75eae7813b88b0fc352788e02cceb9dd677e8bb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=168584397&response-content-disposition=attachment%3B%20filename%3DciFAIR-10.zip&response-content-type=application%2Foctet-stream to data/ciFAIR-10.zip\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 168614301/168614301 [00:00<00:00, 177712263.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting data/ciFAIR-10.zip to data\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "P_mNtjBgEf2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize the model, loss function, optimizer, and other variables\n",
        "mlp = AllCNN()\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 2\n",
        "\n",
        "# Main training loop\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    tolerance = 1e-4\n",
        "    val_acc = []\n",
        "\n",
        "    # Training loop\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        # Calculate loss and perform backpropagation\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store current loss\n",
        "        current_loss.append(loss.item())\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Validation loop\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = mlp(inputs)\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        val_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "    print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "    print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Update tolerance epochs\n",
        "    if update > 0 and update < tolerance:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc) / len(val_acc)"
      ],
      "metadata": {
        "id": "RGP3XTkxEf2V",
        "outputId": "81da96e9-b7d6-4f4f-dbc2-c17f0a0dfc95",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:29:57.658312Z",
          "iopub.execute_input": "2023-11-17T14:29:57.658768Z",
          "iopub.status.idle": "2023-11-17T14:37:39.957635Z",
          "shell.execute_reply.started": "2023-11-17T14:29:57.658732Z",
          "shell.execute_reply": "2023-11-17T14:37:39.956435Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 2.217\nTraining_Acc  : 0.225\nValidation_Acc  : 0.302\nupdate: 0.3024\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.139\nTraining_Acc  : 0.313\nValidation_Acc  : 0.335\nupdate: 0.0329\n--------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "YbKXFHk6Ef2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "id": "kr0jDs6DEf2W",
        "outputId": "8fa2987c-8f9b-4694-8f6e-48bf2b8ef4eb",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:37:39.959183Z",
          "iopub.execute_input": "2023-11-17T14:37:39.959609Z",
          "iopub.status.idle": "2023-11-17T14:37:57.269484Z",
          "shell.execute_reply.started": "2023-11-17T14:37:39.959570Z",
          "shell.execute_reply": "2023-11-17T14:37:57.268427Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test_Acc  : 0.335\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC - ALLConv"
      ],
      "metadata": {
        "id": "EoDsOCKAEf2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "ZCh6VP07Ef2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to perform whitening Transform\n",
        "class WhiteningTransform:\n",
        "    def __init__(self):\n",
        "        self.whitening = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        mean = data.mean(dim=0)\n",
        "        centered_data = data - mean\n",
        "\n",
        "        cov_matrix = torch.mm(centered_data.t(), centered_data) / data.size(0)\n",
        "        U, S, V = torch.svd(cov_matrix)\n",
        "        self.whitening = torch.mm(centered_data, U) / torch.sqrt(S + 1e-10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.whitening is not None:\n",
        "            x = self.whitening.t() @ x.reshape(x.size(0), -1).t()\n",
        "            x = x.t().reshape(x.size(0), 3, 32, 32)\n",
        "            return x\n",
        "        return x\n",
        "\n",
        "# Creating a transform object withwhitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    WhiteningTransform()\n",
        "])\n"
      ],
      "metadata": {
        "id": "Yt39Ds-GEf2X",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:37:57.270891Z",
          "iopub.execute_input": "2023-11-17T14:37:57.271230Z",
          "iopub.status.idle": "2023-11-17T14:37:57.280377Z",
          "shell.execute_reply.started": "2023-11-17T14:37:57.271201Z",
          "shell.execute_reply": "2023-11-17T14:37:57.279312Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loading the MNIST dataset with whitening transformation\n",
        "train_data = ciFAIR10('data', train=True, download=True, transform=transform)\n",
        "test_data = ciFAIR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "total_samples = len(train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# Data Loaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "8QswpPd8Ef2X",
        "outputId": "c7b8a688-73ef-4075-b420-7ae9d1d231aa",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:37:57.281768Z",
          "iopub.execute_input": "2023-11-17T14:37:57.282086Z",
          "iopub.status.idle": "2023-11-17T14:37:59.102504Z",
          "shell.execute_reply.started": "2023-11-17T14:37:57.282059Z",
          "shell.execute_reply": "2023-11-17T14:37:59.101643Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Weight Exponentiation\n",
        "class WeightExponentiation(object):\n",
        "\n",
        "    def __init__(self, epsilon = 5):\n",
        "        self.epsilon = epsilon # epsilon for constraining exponentiation of weights\n",
        "\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if hasattr(module, 'weight'):\n",
        "            # Selecting all other layers except \"first_hidden_layer\"\n",
        "            if \"first_hidden_layer\" not in module.__str__():\n",
        "                w = module.weight.data\n",
        "                # Perform exponentiation operation on negative weights\n",
        "                w[w < 0] = torch.exp(w[w < 0]-self.epsilon)\n",
        "                module.weight.data = w # Storing the updated weights\n"
      ],
      "metadata": {
        "id": "CJKMScDrEf2Y",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:37:59.103656Z",
          "iopub.execute_input": "2023-11-17T14:37:59.103941Z",
          "iopub.status.idle": "2023-11-17T14:37:59.110021Z",
          "shell.execute_reply.started": "2023-11-17T14:37:59.103917Z",
          "shell.execute_reply": "2023-11-17T14:37:59.109060Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "odDBiIkjEf2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the IOC_AllCNN model, loss function, optimizer, and other variables\n",
        "mlp = IOC_AllCNN()\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "epoch = 0\n",
        "prev_val_acc = 0.0\n",
        "tol_epochs = 0\n",
        "max_epochs = 2\n",
        "\n",
        "# Main training loop\n",
        "while epoch < max_epochs and tol_epochs < 2:\n",
        "    epoch += 1\n",
        "    print(f'Epoch {epoch}')\n",
        "    current_loss = []\n",
        "    current_acc = []\n",
        "    tolerance = 1e-4\n",
        "    val_acc = []\n",
        "\n",
        "    # Training loop\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        # Calculate loss and perform backpropagation\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store current loss\n",
        "        current_loss.append(loss.item())\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        pred = torch.max(outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Validation loop\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        inputs, targets = data\n",
        "        val_outputs = mlp(inputs)\n",
        "        pred = torch.max(val_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        val_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Performance Evaluation\n",
        "    print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "    print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "    print('Validation_Acc  : %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "    # Early Stopping Criteria\n",
        "    update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "    print('update: %.4f' % update)\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Update tolerance epochs\n",
        "    if update > 0 and update < tolerance:\n",
        "        tol_epochs += 1\n",
        "    else:\n",
        "        tol_epochs = 0\n",
        "\n",
        "    prev_val_acc = sum(val_acc) / len(val_acc)"
      ],
      "metadata": {
        "id": "IzA-fz_SEf2Y",
        "outputId": "c7842118-fe08-408a-dd29-67a08899268c",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:37:59.111171Z",
          "iopub.execute_input": "2023-11-17T14:37:59.111478Z",
          "iopub.status.idle": "2023-11-17T14:53:38.425129Z",
          "shell.execute_reply.started": "2023-11-17T14:37:59.111453Z",
          "shell.execute_reply": "2023-11-17T14:53:38.424126Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 2.116\nTraining_Acc  : 0.457\nValidation_Acc  : 0.521\nupdate: 0.5210\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.032\nTraining_Acc  : 0.561\nValidation_Acc  : 0.590\nupdate: 0.0688\n--------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation/Testing"
      ],
      "metadata": {
        "id": "Gxy8g82eEf2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = []\n",
        "for i,data in enumerate(test_loader,0):\n",
        "    inputs,targets = data\n",
        "    test_outputs = mlp(inputs)\n",
        "    pred = torch.max(test_outputs,1).indices\n",
        "    acc =(targets==pred).sum().item()\n",
        "    test_acc.append(acc/targets.shape[0])\n",
        "print('Test_Acc  : %.3f'%(sum(test_acc)/len(test_acc)))"
      ],
      "metadata": {
        "id": "d-nq_BwSEf2Z",
        "outputId": "c6f3c2d2-80d9-450f-fb24-1f592ffe34c1",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:53:38.426338Z",
          "iopub.execute_input": "2023-11-17T14:53:38.426681Z",
          "iopub.status.idle": "2023-11-17T14:54:20.928674Z",
          "shell.execute_reply.started": "2023-11-17T14:53:38.426653Z",
          "shell.execute_reply": "2023-11-17T14:54:20.927711Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test_Acc  : 0.580\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp 3. Partially randomized labeling"
      ],
      "metadata": {
        "id": "gmcemkuHB7D1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AllConv"
      ],
      "metadata": {
        "id": "82WSvUgoISAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JTBkyEuFAL5V",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:54:20.929891Z",
          "iopub.execute_input": "2023-11-17T14:54:20.930193Z",
          "iopub.status.idle": "2023-11-17T14:54:20.935184Z",
          "shell.execute_reply.started": "2023-11-17T14:54:20.930167Z",
          "shell.execute_reply": "2023-11-17T14:54:20.934303Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyCIFAR10(Dataset):\n",
        "    def __init__(self, base_dataset, noise_percentage):\n",
        "        \"\"\"\n",
        "        Initialize the NoisyCIFAR10 dataset.\n",
        "\n",
        "        Parameters:\n",
        "            base_dataset (Dataset): Original CIFAR10 dataset.\n",
        "            noise_percentage (float): Percentage of noisy labels to introduce.\n",
        "        \"\"\"\n",
        "        self.base_dataset = base_dataset\n",
        "        self.noise_percentage = noise_percentage\n",
        "        self.labels = np.array(self.base_dataset.dataset.targets)\n",
        "\n",
        "        # Calculate the number of noisy labels to introduce\n",
        "        num_noisy_labels = int(self.noise_percentage * len(self.labels) / 100)\n",
        "\n",
        "        # Randomly choose indices to introduce noise\n",
        "        noisy_indices = np.random.choice(len(self.labels), num_noisy_labels, replace=False)\n",
        "\n",
        "        # Generate random noisy labels\n",
        "        self.noisy_labels = np.random.randint(0, 10, num_noisy_labels)\n",
        "\n",
        "        # Replace original labels with noisy labels at selected indices\n",
        "        self.labels[noisy_indices] = self.noisy_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Length of the base dataset.\n",
        "        \"\"\"\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Get an item from the dataset.\n",
        "\n",
        "        Parameters:\n",
        "            index (int): Index of the item to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "        # Get image and target label from the base dataset\n",
        "        img, target = self.base_dataset[index]\n",
        "\n",
        "        return img, target"
      ],
      "metadata": {
        "id": "oG_rOojXD7G7",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:54:20.936336Z",
          "iopub.execute_input": "2023-11-17T14:54:20.936649Z",
          "iopub.status.idle": "2023-11-17T14:54:20.957281Z",
          "shell.execute_reply.started": "2023-11-17T14:54:20.936622Z",
          "shell.execute_reply": "2023-11-17T14:54:20.956563Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Initialization**: The `NoisyCIFAR10` class takes in the original CIFAR10 dataset (`base_dataset`) and a noise percentage (`noise_percentage`). It calculates the number of noisy labels to introduce based on the percentage provided.\n",
        "\n",
        "- **Introducing Noisy Labels**: It randomly selects indices from the original dataset to introduce noise. Then, it generates random noisy labels for those indices and replaces the original labels with these noisy labels. This step simulates the introduction of label noise into the dataset.\n",
        "\n",
        "- **Data Access**: When accessing elements of the `NoisyCIFAR10` dataset, it retrieves images and their corresponding labels from the base dataset. Some of these labels may have been replaced with noisy labels, depending on the indices selected during initialization.\n",
        "\n",
        "- **Usage**: This class can be used as a drop-in replacement for the original CIFAR10 dataset in PyTorch. It allows researchers and practitioners to experiment with machine learning models in scenarios where noisy labels are present, helping to evaluate model robustness and performance under such conditions."
      ],
      "metadata": {
        "id": "A4jboW9WN5lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Train and Test data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "base_train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Splitting train data into train and validation sets\n",
        "total_samples = len(base_train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "base_train_data, val_data = random_split(base_train_data, [train_size, validation_size])\n",
        "\n",
        "# Define the noise percentages\n",
        "noise_percentages = [20, 40, 60, 80, 100]\n",
        "\n",
        "# Create datasets with different percentages of randomized labels\n",
        "noisy_datasets = [NoisyCIFAR10(base_train_data, percentage) for percentage in noise_percentages]\n",
        "\n",
        "# Data Loaders for all the train, validation, and test sets\n",
        "train_loaders = []\n",
        "for dataset in noisy_datasets:\n",
        "    train_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "    train_loaders.append(train_loader)\n",
        "val_loader = DataLoader(val_data,batch_size=256,shuffle=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=256,shuffle=True)"
      ],
      "metadata": {
        "id": "anHVSVJkB7D1",
        "outputId": "049eb59f-47ff-4e67-c44e-4856a02aa07b",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:54:20.958384Z",
          "iopub.execute_input": "2023-11-17T14:54:20.958648Z",
          "iopub.status.idle": "2023-11-17T14:54:22.590086Z",
          "shell.execute_reply.started": "2023-11-17T14:54:20.958625Z",
          "shell.execute_reply": "2023-11-17T14:54:22.589315Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each index and associated train_loader in enumerate(train_loaders)\n",
        "for index, train_loader in enumerate(train_loaders):\n",
        "    # Initialize the model, loss function, optimizer, and other necessary variables\n",
        "    mlp = AllCNN()\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "    epoch = 0\n",
        "    prev_val_acc = 0.0\n",
        "    tol_epochs = 0\n",
        "    max_epochs = 2\n",
        "\n",
        "    # Training loop\n",
        "    while epoch < max_epochs and tol_epochs < 2:\n",
        "        epoch += 1\n",
        "        print(f'Epoch {epoch}')\n",
        "        current_loss = []\n",
        "        current_acc = []\n",
        "        tolerance = 1e-4\n",
        "        val_acc = []\n",
        "\n",
        "        # Training\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, targets = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = mlp(inputs)\n",
        "\n",
        "            # Compute loss and perform backward pass\n",
        "            loss = loss_fun(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            current_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            pred = torch.max(outputs, 1).indices\n",
        "            acc = (targets == pred).sum().item()\n",
        "            current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "        # Validation\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            inputs, targets = data\n",
        "            val_outputs = mlp(inputs)\n",
        "            pred = torch.max(val_outputs, 1).indices\n",
        "            acc = (targets == pred).sum().item()\n",
        "            val_acc.append(acc / targets.shape[0])\n",
        "\n",
        "        # Performance Evaluation\n",
        "        print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "        print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "        print('Validation_Acc  : %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "        # Early Stopping Criteria\n",
        "        update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "        print('update: %.4f' % update)\n",
        "        print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if 0 < update < tolerance:\n",
        "            tol_epochs += 1\n",
        "        else:\n",
        "            tol_epochs = 0\n",
        "\n",
        "        prev_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    # Testing\n",
        "    test_acc = []\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        inputs, targets = data\n",
        "        test_outputs = mlp(inputs)\n",
        "        pred = torch.max(test_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        test_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Print testing accuracy and noise percentage\n",
        "    print(f'Dataset with {noise_percentages[index]} % noise')\n",
        "    print('Test_Acc  : %.3f' % (sum(test_acc) / len(test_acc)))\n",
        "    print('*********************************************************************************************************')"
      ],
      "metadata": {
        "id": "9woR0P0ZB7D1",
        "outputId": "c410182d-922b-4634-f80a-2c39a541cbd1",
        "execution": {
          "iopub.status.busy": "2023-11-17T14:54:22.591114Z",
          "iopub.execute_input": "2023-11-17T14:54:22.591407Z",
          "iopub.status.idle": "2023-11-17T15:47:57.918452Z",
          "shell.execute_reply.started": "2023-11-17T14:54:22.591380Z",
          "shell.execute_reply": "2023-11-17T15:47:57.917483Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 2.250\nTraining_Acc  : 0.186\nValidation_Acc  : 0.236\nupdate: 0.2360\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.187\nTraining_Acc  : 0.261\nValidation_Acc  : 0.296\nupdate: 0.0599\n--------------------------------------------------------------------------------------------------\nDataset with 20 % noise\nTest_Acc  : 0.284\n*********************************************************************************************************\nEpoch 1\nLoss : 2.257\nTraining_Acc  : 0.179\nValidation_Acc  : 0.239\nupdate: 0.2393\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.203\nTraining_Acc  : 0.244\nValidation_Acc  : 0.270\nupdate: 0.0309\n--------------------------------------------------------------------------------------------------\nDataset with 40 % noise\nTest_Acc  : 0.276\n*********************************************************************************************************\nEpoch 1\nLoss : 2.272\nTraining_Acc  : 0.152\nValidation_Acc  : 0.187\nupdate: 0.1871\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.236\nTraining_Acc  : 0.205\nValidation_Acc  : 0.201\nupdate: 0.0138\n--------------------------------------------------------------------------------------------------\nDataset with 60 % noise\nTest_Acc  : 0.206\n*********************************************************************************************************\nEpoch 1\nLoss : 2.264\nTraining_Acc  : 0.171\nValidation_Acc  : 0.238\nupdate: 0.2382\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.194\nTraining_Acc  : 0.253\nValidation_Acc  : 0.269\nupdate: 0.0307\n--------------------------------------------------------------------------------------------------\nDataset with 80 % noise\nTest_Acc  : 0.273\n*********************************************************************************************************\nEpoch 1\nLoss : 2.263\nTraining_Acc  : 0.171\nValidation_Acc  : 0.215\nupdate: 0.2150\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.203\nTraining_Acc  : 0.241\nValidation_Acc  : 0.266\nupdate: 0.0511\n--------------------------------------------------------------------------------------------------\nDataset with 100 % noise\nTest_Acc  : 0.291\n*********************************************************************************************************\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC-AllConv"
      ],
      "metadata": {
        "id": "fa67ixoWNq8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to perform whitening Transform\n",
        "class WhiteningTransform:\n",
        "    def __init__(self):\n",
        "        self.whitening = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        mean = data.mean(dim=0)\n",
        "        centered_data = data - mean\n",
        "\n",
        "        cov_matrix = torch.mm(centered_data.t(), centered_data) / data.size(0)\n",
        "        U, S, V = torch.svd(cov_matrix)\n",
        "        self.whitening = torch.mm(centered_data, U) / torch.sqrt(S + 1e-10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.whitening is not None:\n",
        "            x = self.whitening.t() @ x.reshape(x.size(0), -1).t()\n",
        "            x = x.t().reshape(x.size(0), 3, 32, 32)\n",
        "            return x\n",
        "        return x\n",
        "\n",
        "# Creating a transform object withwhitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    WhiteningTransform()\n",
        "])"
      ],
      "metadata": {
        "id": "Y9HXlksYNuG6",
        "execution": {
          "iopub.status.busy": "2023-11-17T15:47:57.919647Z",
          "iopub.execute_input": "2023-11-17T15:47:57.919936Z",
          "iopub.status.idle": "2023-11-17T15:47:57.927648Z",
          "shell.execute_reply.started": "2023-11-17T15:47:57.919910Z",
          "shell.execute_reply": "2023-11-17T15:47:57.926780Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Train and Test data\n",
        "base_train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Splitting train data into train and validation sets\n",
        "total_samples = len(base_train_data)\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "train_size = int(train_ratio * total_samples)\n",
        "validation_size = total_samples - train_size\n",
        "\n",
        "base_train_data, val_data = random_split(base_train_data, [train_size, validation_size])\n",
        "\n",
        "# Define the noise percentages\n",
        "noise_percentages = [20, 40, 60, 80, 100]\n",
        "\n",
        "# Create datasets with different percentages of randomized labels\n",
        "noisy_datasets = [NoisyCIFAR10(base_train_data, percentage) for percentage in noise_percentages]\n",
        "\n",
        "# Data Loaders for all the train, validation, and test sets\n",
        "train_loaders = []\n",
        "for dataset in noisy_datasets:\n",
        "    train_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "    train_loaders.append(train_loader)\n",
        "val_loader = DataLoader(val_data,batch_size=256,shuffle=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=256,shuffle=True)"
      ],
      "metadata": {
        "id": "X2W0zKvBOFSg",
        "outputId": "6fc42fb4-4fc5-417b-f74b-a7fdda21a22f",
        "execution": {
          "iopub.status.busy": "2023-11-17T15:47:57.928922Z",
          "iopub.execute_input": "2023-11-17T15:47:57.929494Z",
          "iopub.status.idle": "2023-11-17T15:47:59.620518Z",
          "shell.execute_reply.started": "2023-11-17T15:47:57.929468Z",
          "shell.execute_reply": "2023-11-17T15:47:59.619566Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Iterate through each training loader\n",
        "for index, train_loader in enumerate(train_loaders):\n",
        "    # Initialize model, loss function, optimizer, and other variables\n",
        "    mlp = IOC_AllCNN()\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "    epoch = 0\n",
        "    prev_val_acc = 0.0\n",
        "    tol_epochs = 0\n",
        "    max_epochs = 2\n",
        "\n",
        "    # Training loop\n",
        "    while epoch < max_epochs and tol_epochs < 2:\n",
        "        epoch += 1\n",
        "        print(f'Epoch {epoch}')\n",
        "        current_loss = []\n",
        "        current_acc = []\n",
        "        tolerance = 1e-4\n",
        "        val_acc = []\n",
        "\n",
        "        # Training\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, targets = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = mlp(inputs)\n",
        "\n",
        "            # Calculate loss and perform backpropagation\n",
        "            loss = loss_fun(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Apply weight exponentiation\n",
        "            mlp.apply(WeightExponentiation())\n",
        "            current_loss.append(loss.item())\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            pred = torch.max(outputs, 1).indices\n",
        "            acc = (targets == pred).sum().item()\n",
        "            current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "        # Validation loop (assuming val_loader is defined somewhere)\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            inputs, targets = data\n",
        "            val_outputs = mlp(inputs)\n",
        "            pred = torch.max(val_outputs, 1).indices\n",
        "            acc = (targets == pred).sum().item()\n",
        "            val_acc.append(acc / targets.shape[0])\n",
        "\n",
        "        # Performance Evaluation\n",
        "        print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "        print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "        print('Validation_Acc  : %.3f' % (sum(val_acc) / len(val_acc)))\n",
        "\n",
        "        # Early Stopping Criteria\n",
        "        update = (sum(val_acc) / len(val_acc) - prev_val_acc)\n",
        "        print('update: %.4f' % update)\n",
        "        print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if update > 0 and update < tolerance:\n",
        "            tol_epochs += 1\n",
        "        else:\n",
        "            tol_epochs = 0\n",
        "\n",
        "        prev_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    # Testing loop\n",
        "    test_acc = []\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        inputs, targets = data\n",
        "        test_outputs = mlp(inputs)\n",
        "        pred = torch.max(test_outputs, 1).indices\n",
        "        acc = (targets == pred).sum().item()\n",
        "        test_acc.append(acc / targets.shape[0])\n",
        "\n",
        "    # Print results for the current dataset\n",
        "    print(f'Dataset with {noise_percentages[index]} % noise')\n",
        "    print('Test_Acc  : %.3f' % (sum(test_acc) / len(test_acc)))\n",
        "    print('*********************************************************************************************************')\n"
      ],
      "metadata": {
        "id": "YMvUqVAQOV7q",
        "outputId": "8770708d-cb8f-44b4-a643-fc898a03fe7b",
        "execution": {
          "iopub.status.busy": "2023-11-17T15:47:59.621824Z",
          "iopub.execute_input": "2023-11-17T15:47:59.622128Z",
          "iopub.status.idle": "2023-11-17T17:26:18.065581Z",
          "shell.execute_reply.started": "2023-11-17T15:47:59.622102Z",
          "shell.execute_reply": "2023-11-17T17:26:18.064520Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\nLoss : 2.128\nTraining_Acc  : 0.451\nValidation_Acc  : 0.533\nupdate: 0.5331\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.047\nTraining_Acc  : 0.564\nValidation_Acc  : 0.582\nupdate: 0.0493\n--------------------------------------------------------------------------------------------------\nDataset with 20 % noise\nTest_Acc  : 0.579\n*********************************************************************************************************\nEpoch 1\nLoss : 2.130\nTraining_Acc  : 0.443\nValidation_Acc  : 0.523\nupdate: 0.5229\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.048\nTraining_Acc  : 0.561\nValidation_Acc  : 0.588\nupdate: 0.0651\n--------------------------------------------------------------------------------------------------\nDataset with 40 % noise\nTest_Acc  : 0.580\n*********************************************************************************************************\nEpoch 1\nLoss : 2.126\nTraining_Acc  : 0.451\nValidation_Acc  : 0.530\nupdate: 0.5296\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.045\nTraining_Acc  : 0.567\nValidation_Acc  : 0.580\nupdate: 0.0505\n--------------------------------------------------------------------------------------------------\nDataset with 60 % noise\nTest_Acc  : 0.574\n*********************************************************************************************************\nEpoch 1\nLoss : 2.127\nTraining_Acc  : 0.450\nValidation_Acc  : 0.531\nupdate: 0.5311\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.049\nTraining_Acc  : 0.561\nValidation_Acc  : 0.575\nupdate: 0.0438\n--------------------------------------------------------------------------------------------------\nDataset with 80 % noise\nTest_Acc  : 0.567\n*********************************************************************************************************\nEpoch 1\nLoss : 2.126\nTraining_Acc  : 0.458\nValidation_Acc  : 0.542\nupdate: 0.5418\n--------------------------------------------------------------------------------------------------\nEpoch 2\nLoss : 2.045\nTraining_Acc  : 0.567\nValidation_Acc  : 0.604\nupdate: 0.0619\n--------------------------------------------------------------------------------------------------\nDataset with 100 % noise\nTest_Acc  : 0.593\n*********************************************************************************************************\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 4. Boosted Ensemble"
      ],
      "metadata": {
        "id": "KmDuy9vwARGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOC-AllConv"
      ],
      "metadata": {
        "id": "eb6QSNEKJYlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:37.355763Z",
          "iopub.execute_input": "2023-11-20T13:41:37.356169Z",
          "iopub.status.idle": "2023-11-20T13:41:37.430713Z",
          "shell.execute_reply.started": "2023-11-20T13:41:37.356143Z",
          "shell.execute_reply": "2023-11-20T13:41:37.429756Z"
        },
        "trusted": true,
        "id": "CQnCML6wKZGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a transform object with whitening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "   WhiteningTransform()\n",
        "])\n",
        "# Loading the CIFAR10 dataset with whitening transformation\n",
        "train_data = CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10('data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "_sCY8Yr-5X25",
        "outputId": "378a2da0-ce7c-450a-8d1e-6a0b5cd32816",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:37.454863Z",
          "iopub.execute_input": "2023-11-20T13:41:37.455215Z",
          "iopub.status.idle": "2023-11-20T13:41:44.921383Z",
          "shell.execute_reply.started": "2023-11-20T13:41:37.455167Z",
          "shell.execute_reply": "2023-11-20T13:41:44.920401Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 170498071/170498071 [00:03<00:00, 43428993.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Weight Exponentiation\n",
        "class WeightExponentiation(object):\n",
        "\n",
        "    def __init__(self, epsilon = 5):\n",
        "        self.epsilon = epsilon # epsilon for constraining exponentiation of weights\n",
        "\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if hasattr(module, 'weight'):\n",
        "            # Selecting all other layers except \"first_hidden_layer\"\n",
        "            if \"first_hidden_layer\" not in module.__str__() and \"fc0\" not in module.__str__():\n",
        "                w = module.weight.data\n",
        "                # Perform exponentiation operation on negative weights\n",
        "                w[w < 0] = torch.exp(w[w < 0]-self.epsilon)\n",
        "                module.weight.data = w # Storing the updated weights\n"
      ],
      "metadata": {
        "id": "05dDBG1X6Ee4",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.922544Z",
          "iopub.execute_input": "2023-11-20T13:41:44.922832Z",
          "iopub.status.idle": "2023-11-20T13:41:44.929437Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.922807Z",
          "shell.execute_reply": "2023-11-20T13:41:44.928612Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class ExpertEnsemble(nn.Module):\n",
        "    def __init__(self, base_learner, num_experts: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize ExpertEnsemble.\n",
        "\n",
        "        Parameters:\n",
        "            base_learner (nn.Module): Base learner model.\n",
        "            num_experts (int): Number of experts in the ensemble.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.experts = nn.ModuleList([base_learner().to(device) for _ in range(self.num_experts)])\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the ensemble.\n",
        "\n",
        "        Parameters:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            list: List of outputs from individual experts.\n",
        "        \"\"\"\n",
        "        expert_outputs = [expert(x) for expert in self.experts]\n",
        "        return expert_outputs\n",
        "\n",
        "    def train_experts(self, train_loader):\n",
        "        \"\"\"\n",
        "        Train individual experts in the ensemble.\n",
        "\n",
        "        Parameters:\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "        \"\"\"\n",
        "        weights = torch.ones(len(train_loader.dataset))\n",
        "        for i in range(self.num_experts):\n",
        "            print(\"Expert:\", i)\n",
        "            self.train_single_expert(self.experts[i], train_loader)\n",
        "            weights = self.update_sample_weights(self.experts[i], train_loader.dataset, weights)\n",
        "            train_loader = self.bootstrap_dataloader(train_loader, weights)\n",
        "\n",
        "    def train_single_expert(self, model, train_loader):\n",
        "        \"\"\"\n",
        "        Train a single expert in the ensemble.\n",
        "\n",
        "        Parameters:\n",
        "            model (nn.Module): Expert model to train.\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "        \"\"\"\n",
        "        model.train()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=[0.9, 0.9])\n",
        "        max_epochs = 2\n",
        "        for epoch in range(max_epochs):\n",
        "            print(f'Epoch {epoch + 1}')\n",
        "            current_loss = []\n",
        "            current_acc = []\n",
        "            for i, data in enumerate(train_loader, 0):\n",
        "                inputs, targets = data\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                model.apply(WeightExponentiation())\n",
        "                current_loss.append(loss.item())\n",
        "                pred = torch.max(outputs, 1).indices\n",
        "                acc = (targets == pred).sum().item()\n",
        "                current_acc.append(acc / targets.shape[0])\n",
        "            print('Loss: %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "            print('Training Accuracy: %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "\n",
        "    def update_sample_weights(self, weak_learner, dataset, sample_weights, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        Update sample weights based on the performance of a weak learner.\n",
        "\n",
        "        Parameters:\n",
        "            weak_learner (nn.Module): Weak learner model.\n",
        "            dataset (torch.utils.data.Dataset): Dataset used for training.\n",
        "            sample_weights (torch.Tensor): Current sample weights.\n",
        "            learning_rate (float): Learning rate for updating sample weights.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Updated sample weights.\n",
        "        \"\"\"\n",
        "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            weak_learner.to(device)\n",
        "            outputs = weak_learner(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            misclassifications = (outputs.argmax(dim=1) != labels).float()\n",
        "            sample_weights = sample_weights.to(device)\n",
        "            sample_weights *= torch.exp(learning_rate * misclassifications)\n",
        "            sample_weights /= sample_weights.sum()\n",
        "        return sample_weights\n",
        "\n",
        "    def bootstrap_dataloader(self, dataloader, sample_weights):\n",
        "        \"\"\"\n",
        "        Create a new DataLoader by bootstrapping from the original DataLoader based on sample weights.\n",
        "\n",
        "        Parameters:\n",
        "            dataloader (DataLoader): Original DataLoader.\n",
        "            sample_weights (torch.Tensor): Sample weights used for bootstrapping.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: Bootstrapped DataLoader.\n",
        "        \"\"\"\n",
        "        num_samples = len(dataloader.dataset)\n",
        "        bootstrap_indices = torch.multinomial(sample_weights, num_samples, replacement=True).int()\n",
        "        return DataLoader([dataloader.dataset[i] for i in bootstrap_indices])"
      ],
      "metadata": {
        "id": "cRK9EDGwOeQw",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.953986Z",
          "iopub.execute_input": "2023-11-20T13:41:44.954334Z",
          "iopub.status.idle": "2023-11-20T13:41:44.974792Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.954301Z",
          "shell.execute_reply": "2023-11-20T13:41:44.973736Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoostingEnsemble:\n",
        "    def __init__(self, num_experts, base_learner, gating_network, train_loader, test_loader):\n",
        "        \"\"\"\n",
        "        Initialize the Boosting Ensemble.\n",
        "\n",
        "        Parameters:\n",
        "            num_experts (int): Number of experts in the ensemble.\n",
        "            base_learner (nn.Module): Base learner model.\n",
        "            gating_network (nn.Module): Gating network model.\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "            test_loader (DataLoader): DataLoader for testing data.\n",
        "        \"\"\"\n",
        "        self.num_experts = num_experts\n",
        "        self.base_learner = base_learner\n",
        "        self.gating_network = gating_network(num_experts).to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "    def expert_training(self):\n",
        "        \"\"\"\n",
        "        Train individual experts in the ensemble.\n",
        "        \"\"\"\n",
        "        model = ExpertEnsemble(self.base_learner, self.num_experts)\n",
        "        model.train_experts(self.train_loader)\n",
        "        print(\"Expert Training Completed\")\n",
        "        self.trained_experts = model.experts\n",
        "\n",
        "        # Set requires_grad=True for expert parameters\n",
        "        for expert in self.trained_experts:\n",
        "            for param in expert.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def gn_training(self):\n",
        "        \"\"\"\n",
        "        Train the gating network.\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.Adam(self.gating_network.parameters(), lr=1e-4, betas=[0.9, 0.9])\n",
        "        max_epochs = 5\n",
        "        loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            print(f'Epoch {epoch + 1}')\n",
        "            current_loss = []\n",
        "            current_acc = []\n",
        "\n",
        "            # Training\n",
        "            for i, data in enumerate(self.train_loader, 0):\n",
        "                inputs, targets = data\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Combine outputs of individual experts\n",
        "                combined_outputs = torch.cat([expert(inputs) for expert in self.trained_experts], dim=1)\n",
        "\n",
        "                # Forward pass through the gating network\n",
        "                outputs = self.gating_network(combined_outputs)\n",
        "                loss = loss_fun(outputs, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                self.gating_network.apply(WeightExponentiation())\n",
        "                current_loss.append(loss.item())\n",
        "\n",
        "                pred = torch.max(outputs, 1).indices\n",
        "                acc = (targets == pred).sum().item()\n",
        "                current_acc.append(acc / targets.shape[0])\n",
        "\n",
        "            # Performance Evaluation\n",
        "            print('Loss : %.3f' % (sum(current_loss) / len(current_loss)))\n",
        "            print('Training_Acc  : %.3f' % (sum(current_acc) / len(current_acc)))\n",
        "\n",
        "        print(\"Training of Gating Network Completed\")\n",
        "\n",
        "    def testing(self):\n",
        "        \"\"\"\n",
        "        Test the ensemble model.\n",
        "        \"\"\"\n",
        "        test_acc = []\n",
        "        for i, data in enumerate(self.test_loader, 0):\n",
        "            inputs, targets = data\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Combine outputs of individual experts\n",
        "            combined_outputs = torch.cat([expert(inputs) for expert in self.trained_experts], dim=1)\n",
        "\n",
        "            # Forward pass through the gating network\n",
        "            test_outputs = self.gating_network(combined_outputs)\n",
        "\n",
        "            pred = torch.max(test_outputs, 1).indices\n",
        "            acc = (targets == pred).sum().item()\n",
        "            test_acc.append(acc / targets.shape[0])\n",
        "\n",
        "        print('Test_Acc  : %.3f' % (sum(test_acc) / len(test_acc)))"
      ],
      "metadata": {
        "id": "Bd1pC0D55N3o",
        "execution": {
          "iopub.status.busy": "2023-11-20T13:41:44.976007Z",
          "iopub.execute_input": "2023-11-20T13:41:44.976280Z",
          "iopub.status.idle": "2023-11-20T13:41:45.000598Z",
          "shell.execute_reply.started": "2023-11-20T13:41:44.976256Z",
          "shell.execute_reply": "2023-11-20T13:41:44.999605Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Gating(nn.Module):\n",
        "    def __init__(self, n_experts: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the CNN_Gating module.\n",
        "\n",
        "        Parameters:\n",
        "            n_experts (int): Number of experts.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Add extra filters to the first convolutional layer\n",
        "        self.first_conv_layer = nn.Conv2d(3, 192, 3)\n",
        "\n",
        "        # Define layers\n",
        "        self.layers = nn.Sequential(\n",
        "            # Fully connected layer to reshape input\n",
        "            nn.Linear(3 * 10, 3 * 32 * 32),  # Assuming input size of 3x10 and output size of 3x32x32\n",
        "            Reshape((-1, 3, 32, 32)),  # Reshape to (batch_size, 3, 32, 32)\n",
        "\n",
        "            self.first_conv_layer,\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(192, 96, 3),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(96, 96, 3, 2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(96, 192, 3),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(192, 192, 1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ELU(True),\n",
        "\n",
        "            nn.Conv2d(192, 10, 1),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ELU(True)\n",
        "        )\n",
        "\n",
        "        # Adaptive average pooling\n",
        "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Softmax layer\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the CNN_Gating module.\n",
        "\n",
        "        Parameters:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after softmax.\n",
        "        \"\"\"\n",
        "        x = self.layers(x)\n",
        "        x = self.pooling(x)\n",
        "        x = x.squeeze()\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "CutoUpRrxe_k",
        "execution": {
          "iopub.status.busy": "2023-11-20T11:58:47.478563Z",
          "iopub.execute_input": "2023-11-20T11:58:47.479401Z",
          "iopub.status.idle": "2023-11-20T11:58:47.489569Z",
          "shell.execute_reply.started": "2023-11-20T11:58:47.479369Z",
          "shell.execute_reply": "2023-11-20T11:58:47.488450Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Boosting_Ensemble(3,IOC_AllCNN,CNN_Gating,train_loader)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-20T11:58:48.159350Z",
          "iopub.execute_input": "2023-11-20T11:58:48.160044Z",
          "iopub.status.idle": "2023-11-20T11:59:48.549194Z",
          "shell.execute_reply.started": "2023-11-20T11:58:48.160004Z",
          "shell.execute_reply": "2023-11-20T11:59:48.547622Z"
        },
        "trusted": true,
        "id": "DLlytz1zKZGw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}